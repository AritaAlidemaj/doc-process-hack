{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 02: InsightExtractor: Leveraging Azure Document Intelligence for Data Retrieval\n",
    "\n",
    "This notebook demonstrates how to upload data to Azure Blob Storage, process the data using Azure Document Intelligence, and retrieve the information from these documents in JSON format. The steps include:\n",
    "\n",
    "1. **Upload Data to Blob Storage**: We will upload documents to Azure Blob Storage for processing.\n",
    "2. **Process Data with Azure Document Intelligence**: Utilize Azure's Document Intelligence capabilities to analyze and extract information from the uploaded documents.\n",
    "3. **Retrieve Information in JSON Format**: Extract and retrieve the processed information in JSON format for further use.\n",
    "\n",
    "## Importance of Document Processing\n",
    "\n",
    "Automating document processing is crucial for improving efficiency and accuracy in handling large volumes of data. By leveraging Azure's cloud services, organizations can streamline their workflows, reduce manual errors, and gain valuable insights from their documents. This approach not only saves time and resources but also enhances data accessibility and decision-making capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- Upload Data to Blob\n",
    "\n",
    "In this step, we will upload our documents to Azure Blob Storage, which serves as a scalable and secure storage solution for our data. The data used is stored in this repo's *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded readme.md to blob storage.\n",
      "Uploaded loanagreements/la_janesmith.pdf to blob storage.\n",
      "Uploaded loanform/lp_janesmith.pdf to blob storage.\n",
      "Uploaded paystubs/paystubjanesmith.pdf to blob storage.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the connection string and data folder from the environment variables\n",
    "connection_string = os.getenv('connection_string')\n",
    "data_folder = os.getenv('data_folder')\n",
    "container_name = os.getenv('container_name')\n",
    "\n",
    "# Ensure the connection string, data folder, and container name are not None\n",
    "if connection_string is None:\n",
    "    raise ValueError(\"The connection string environment variable is not set.\")\n",
    "if data_folder is None:\n",
    "    raise ValueError(\"The data folder environment variable is not set.\")\n",
    "if container_name is None:\n",
    "    raise ValueError(\"The container name environment variable is not set.\")\n",
    "\n",
    "# Ensure the data folder exists\n",
    "if not os.path.isdir(data_folder):\n",
    "    raise FileNotFoundError(f\"The specified data folder does not exist: {data_folder}\")\n",
    "\n",
    "# Create a BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Upload files in the data folder and its subdirectories to the blob container\n",
    "for root, dirs, files in os.walk(data_folder):\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            # Create a blob path that maintains the directory structure\n",
    "            blob_path = os.path.relpath(file_path, data_folder).replace(\"\\\\\", \"/\")\n",
    "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
    "            with open(file_path, \"rb\") as data:\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "            print(f\"Uploaded {blob_path} to blob storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Process Data with Azure Document Intelligence\n",
    "\n",
    "In this step, we will use Azure Document Intelligence to analyze and extract information from the uploaded documents. The code demonstrates how to authenticate with Azure, submit a document for analysis, and retrieve the results, including detected languages, lines, words, and paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import Required Libraries and Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature, AnalyzeResult, AnalyzeDocumentRequest\n",
    "\n",
    "def generate_sas_url(blob_service_client, container_name, blob_name, expiry_hours=1):\n",
    "    \"\"\"\n",
    "    Generate a SAS URL for a blob in Azure Blob Storage.\n",
    "\n",
    "    :param blob_service_client: BlobServiceClient instance\n",
    "    :param container_name: Name of the container\n",
    "    :param blob_name: Name of the blob\n",
    "    :param expiry_hours: Expiry time in hours for the SAS token\n",
    "    :return: SAS URL for the blob\n",
    "    \"\"\"\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_service_client.account_name,\n",
    "        container_name=container_name,\n",
    "        blob_name=blob_name,\n",
    "        account_key=blob_service_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(hours=expiry_hours)\n",
    "    )\n",
    "\n",
    "    sas_url = f\"https://{blob_service_client.account_name}.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n",
    "    return sas_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Helper Functions for Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(page, line):\n",
    "    result = []\n",
    "    for word in page.words:\n",
    "        if _in_span(word, line.spans):\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "def _in_span(word, spans):\n",
    "    for span in spans:\n",
    "        if word.span.offset >= span.offset and (word.span.offset + word.span.length) <= (span.offset + span.length):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def bounding_region_to_dict(region):\n",
    "    return {\n",
    "        \"page_number\": region.page_number,\n",
    "        \"polygon\": [point for point in region.polygon]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyze Document Using Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layout(sas_url):\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\"), credential=AzureKeyCredential(os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", AnalyzeDocumentRequest(url_source=sas_url)\n",
    "    )\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    analysis_result = {\n",
    "        \"handwritten\": any([style.is_handwritten for style in result.styles]) if result.styles else False,\n",
    "        \"pages\": [],\n",
    "        \"tables\": []\n",
    "    }\n",
    "\n",
    "    for page in result.pages:\n",
    "        page_info = {\n",
    "            \"page_number\": page.page_number,\n",
    "            \"width\": page.width,\n",
    "            \"height\": page.height,\n",
    "            \"unit\": page.unit,\n",
    "            \"lines\": [],\n",
    "            \"selection_marks\": []\n",
    "        }\n",
    "\n",
    "        if page.lines:\n",
    "            for line in page.lines:\n",
    "                line_info = {\n",
    "                    \"text\": line.content,\n",
    "                    \"polygon\": line.polygon,\n",
    "                    \"words\": [{\"content\": word.content, \"confidence\": word.confidence} for word in get_words(page, line)]\n",
    "                }\n",
    "                page_info[\"lines\"].append(line_info)\n",
    "\n",
    "        if page.selection_marks:\n",
    "            for selection_mark in page.selection_marks:\n",
    "                selection_mark_info = {\n",
    "                    \"state\": selection_mark.state,\n",
    "                    \"polygon\": selection_mark.polygon,\n",
    "                    \"confidence\": selection_mark.confidence\n",
    "                }\n",
    "                page_info[\"selection_marks\"].append(selection_mark_info)\n",
    "\n",
    "        analysis_result[\"pages\"].append(page_info)\n",
    "\n",
    "    if result.tables:\n",
    "        for table in result.tables:\n",
    "            table_info = {\n",
    "                \"row_count\": table.row_count,\n",
    "                \"column_count\": table.column_count,\n",
    "                \"bounding_regions\": [{\"page_number\": region.page_number, \"polygon\": region.polygon} for region in table.bounding_regions] if table.bounding_regions else [],\n",
    "                \"cells\": [{\"row_index\": cell.row_index, \"column_index\": cell.column_index, \"content\": cell.content, \"bounding_regions\": [{\"page_number\": region.page_number, \"polygon\": region.polygon} for region in cell.bounding_regions] if cell.bounding_regions else []} for cell in table.cells]\n",
    "            }\n",
    "            analysis_result[\"tables\"].append(table_info)\n",
    "\n",
    "    return analysis_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Save Analysis Results to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_analysis_results(blob_service_client, container_name, blob_name, analysis_results):\n",
    "    if analysis_results is None:\n",
    "        print(f\"No analysis results for {blob_name}. Skipping save.\")\n",
    "        return\n",
    "\n",
    "    # Define the name for the results file\n",
    "    results_blob_name = f\"{blob_name}_results.json\"\n",
    "\n",
    "    # Convert the analysis results to JSON\n",
    "    results_json = json.dumps(analysis_results, indent=2)\n",
    "\n",
    "    # Upload the results to the blob\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=results_blob_name)\n",
    "    blob_client.upload_blob(results_json, overwrite=True)\n",
    "\n",
    "    print(f\"Saved analysis results to {results_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing blob: loanagreements/la_janesmith.pdf\n",
      "Generated SAS URL: https://stgweaihack.blob.core.windows.net/bankdetail/loanagreements/la_janesmith.pdf?se=2024-09-08T15%3A55%3A31Z&sp=r&sv=2021-08-06&sr=b&sig=7h%2BH%2Blqu2gZtCB9GrIOpTtBxIKkbcxhtrrn8A8FmKgA%3D\n",
      "Saved analysis results to loanagreements/la_janesmith.pdf_results.json\n",
      "Processing blob: loanagreements/la_janesmith.pdf_results.json\n",
      "Skipping unsupported file format: loanagreements/la_janesmith.pdf_results.json\n",
      "Processing blob: loanform/lp_janesmith.pdf\n",
      "Generated SAS URL: https://stgweaihack.blob.core.windows.net/bankdetail/loanform/lp_janesmith.pdf?se=2024-09-08T15%3A55%3A38Z&sp=r&sv=2021-08-06&sr=b&sig=mMX7y3b573lWu0nGt/UDHqCzpA9g0OQ/USiQyS/KIlc%3D\n",
      "Saved analysis results to loanform/lp_janesmith.pdf_results.json\n",
      "Processing blob: loanform/lp_janesmith.pdf_results.json\n",
      "Skipping unsupported file format: loanform/lp_janesmith.pdf_results.json\n",
      "Processing blob: paystubs/paystubjanesmith.pdf\n",
      "Generated SAS URL: https://stgweaihack.blob.core.windows.net/bankdetail/paystubs/paystubjanesmith.pdf?se=2024-09-08T15%3A55%3A43Z&sp=r&sv=2021-08-06&sr=b&sig=q551BV2V2lHq3w/rSJWo1YGcKk2gQ%2BVbj4QuLBloxPQ%3D\n",
      "Saved analysis results to paystubs/paystubjanesmith.pdf_results.json\n",
      "Processing blob: paystubs/paystubjanesmith.pdf_results.json\n",
      "Skipping unsupported file format: paystubs/paystubjanesmith.pdf_results.json\n",
      "Processing blob: processed_document.txt\n",
      "Skipping unsupported file format: processed_document.txt\n",
      "Processing blob: readme.md\n",
      "Skipping unsupported file format: readme.md\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Retrieve the connection string and container name from the environment variables\n",
    "    connection_string = os.getenv('connection_string')\n",
    "    container_name = os.getenv('container_name')\n",
    "\n",
    "    # Ensure the connection string is not None\n",
    "    if connection_string is None:\n",
    "        raise ValueError(\"The connection string environment variable is not set.\")\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # List all blobs in the container\n",
    "    blob_list = blob_service_client.get_container_client(container_name).list_blobs()\n",
    "\n",
    "    # Iterate over each blob\n",
    "    for blob in blob_list:\n",
    "        blob_name = blob.name\n",
    "        print(f\"Processing blob: {blob_name}\")\n",
    "\n",
    "        # Ensure the file format is supported\n",
    "        supported_formats = ['.pdf', '.jpeg', '.jpg', '.png', '.tiff']\n",
    "        if not any(blob_name.lower().endswith(ext) for ext in supported_formats):\n",
    "            print(f\"Skipping unsupported file format: {blob_name}\")\n",
    "            continue\n",
    "\n",
    "        # Generate the SAS URL\n",
    "        sas_url = generate_sas_url(blob_service_client, container_name, blob_name)\n",
    "        print(f\"Generated SAS URL: {sas_url}\")\n",
    "\n",
    "        # Call the analyze_layout function with the SAS URL\n",
    "        analysis_results = analyze_layout(sas_url)\n",
    "\n",
    "        # Save the analysis results\n",
    "        save_analysis_results(blob_service_client, container_name, blob_name, analysis_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
