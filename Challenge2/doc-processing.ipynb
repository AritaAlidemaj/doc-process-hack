{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing with Azure\n",
    "\n",
    "This notebook demonstrates how to upload data to Azure Blob Storage, process the data using Azure Document Intelligence, and retrieve the information from these documents in JSON format. The steps include:\n",
    "\n",
    "1. **Upload Data to Blob Storage**: We will upload documents to Azure Blob Storage for processing.\n",
    "2. **Process Data with Azure Document Intelligence**: Utilize Azure's Document Intelligence capabilities to analyze and extract information from the uploaded documents.\n",
    "3. **Retrieve Information in JSON Format**: Extract and retrieve the processed information in JSON format for further use.\n",
    "\n",
    "## Importance of Document Processing\n",
    "\n",
    "Automating document processing is crucial for improving efficiency and accuracy in handling large volumes of data. By leveraging Azure's cloud services, organizations can streamline their workflows, reduce manual errors, and gain valuable insights from their documents. This approach not only saves time and resources but also enhances data accessibility and decision-making capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- Upload Data to Blob\n",
    "\n",
    "In this step, we will upload our documents to Azure Blob Storage, which serves as a scalable and secure storage solution for our data. The data used is stored in this repo's *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "\n",
    "# Create a BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(os.getenv('connection_string'))\n",
    "\n",
    "# Create a container if it doesn't exist\n",
    "container_client = blob_service_client.get_container_client(os.getenv('container_name'))\n",
    "try:\n",
    "    container_client.create_container()\n",
    "except Exception as e:\n",
    "    print(f\"Container already exists: {e}\")\n",
    "\n",
    "# Upload files in the data folder to the blob container\n",
    "for filename in os.listdir(os.getenv('data_folder')):\n",
    "    file_path = os.path.join(os.getenv('data_folder'), filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        blob_client = blob_service_client.get_blob_client(container=os.getenv('container_name'), blob=filename)\n",
    "        with open(file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"Uploaded {filename} to blob storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Process Data with Azure Document Intelligence\n",
    "\n",
    "In this step, we will use Azure Document Intelligence to analyze and extract information from the uploaded documents. The code demonstrates how to authenticate with Azure, submit a document for analysis, and retrieve the results, including detected languages, lines, words, and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(page, line):\n",
    "    result = []\n",
    "    for word in page.words:\n",
    "        if _in_span(word, line.spans):\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "# To learn the detailed concept of \"span\" in the following codes, visit: https://aka.ms/spans \n",
    "def _in_span(word, spans):\n",
    "    for span in spans:\n",
    "        if word.span.offset >= span.offset and (word.span.offset + word.span.length) <= (span.offset + span.length):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def analyze_read():\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "    from azure.ai.documentintelligence.models import DocumentAnalysisFeature, AnalyzeResult, AnalyzeDocumentRequest\n",
    "\n",
    "    # For how to obtain the endpoint and key, please see PREREQUISITES above.\n",
    "    endpoint = os.environ[\"DOCUMENTINTELLIGENCE_ENDPOINT\"]\n",
    "    key = os.environ[\"DOCUMENTINTELLIGENCE_API_KEY\"]\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "    # Analyze a document at a URL:\n",
    "    formUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/rest-api/read.png\"\n",
    "    # Replace with your actual formUrl:\n",
    "    # If you use the URL of a public website, to find more URLs, please visit: https://aka.ms/more-URLs \n",
    "    # If you analyze a document in Blob Storage, you need to generate Public SAS URL, please visit: https://aka.ms/create-sas-tokens\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-read\",\n",
    "        AnalyzeDocumentRequest(url_source=formUrl),\n",
    "        features=[DocumentAnalysisFeature.LANGUAGES]\n",
    "    )       \n",
    "    \n",
    "    # # If analyzing a local document, remove the comment markers (#) at the beginning of these 11 lines.\n",
    "    # # Delete or comment out the part of \"Analyze a document at a URL\" above.\n",
    "    # # Replace <path to your sample file>  with your actual file path.\n",
    "    # path_to_sample_document = \"<path to your sample file>\"\n",
    "    # with open(path_to_sample_document, \"rb\") as f:\n",
    "    #     poller = document_intelligence_client.begin_analyze_document(\n",
    "    #         \"prebuilt-read\",\n",
    "    #         analyze_request=f,\n",
    "    #         features=[DocumentAnalysisFeature.LANGUAGES],\n",
    "    #         content_type=\"application/octet-stream\",\n",
    "    #     )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    \n",
    "    # [START analyze_read]\n",
    "    # Detect languages.\n",
    "    print(\"----Languages detected in the document----\")\n",
    "    if result.languages is not None:\n",
    "        for language in result.languages:\n",
    "            print(f\"Language code: '{language.locale}' with confidence {language.confidence}\")\n",
    "    \n",
    "    # To learn the detailed concept of \"bounding polygon\" in the following content, visit: https://aka.ms/bounding-region\n",
    "    # Analyze pages.\n",
    "    for page in result.pages:\n",
    "        print(f\"----Analyzing document from page #{page.page_number}----\")\n",
    "        print(f\"Page has width: {page.width} and height: {page.height}, measured with unit: {page.unit}\")\n",
    "\n",
    "        # Analyze lines.\n",
    "        if page.lines:\n",
    "            for line_idx, line in enumerate(page.lines):\n",
    "                words = get_words(page, line)\n",
    "                print(\n",
    "                    f\"...Line # {line_idx} has {len(words)} words and text '{line.content}' within bounding polygon '{line.polygon}'\"\n",
    "                )\n",
    "\n",
    "                # Analyze words.\n",
    "                for word in words:\n",
    "                    print(f\"......Word '{word.content}' has a confidence of {word.confidence}\")\n",
    "        \n",
    "    # Analyze paragraphs.\n",
    "    if result.paragraphs:\n",
    "        print(f\"----Detected #{len(result.paragraphs)} paragraphs in the document----\")\n",
    "        for paragraph in result.paragraphs:\n",
    "            print(f\"Found paragraph within {paragraph.bounding_regions} bounding region\")\n",
    "            print(f\"...with content: '{paragraph.content}'\")\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    # [END analyze_read]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from azure.core.exceptions import HttpResponseError\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "    try:\n",
    "        load_dotenv(find_dotenv())\n",
    "        analyze_read()\n",
    "    except HttpResponseError as error:\n",
    "        # Examples of how to check an HttpResponseError\n",
    "        # Check by error code:\n",
    "        if error.error is not None:\n",
    "            if error.error.code == \"InvalidImage\":\n",
    "                print(f\"Received an invalid image error: {error.error}\")\n",
    "            if error.error.code == \"InvalidRequest\":\n",
    "                print(f\"Received an invalid request error: {error.error}\")\n",
    "            # Raise the error again after printing it\n",
    "            raise\n",
    "        # If the inner error is None and then it is possible to check the message to get more information:\n",
    "        if \"Invalid request\".casefold() in error.message.casefold():\n",
    "            print(f\"Uh-oh! Seems there was an invalid request: {error}\")\n",
    "        # Raise the error again\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Retrieve Information in JSON Format\n",
    "\n",
    "In this step, we will extract and retrieve the processed information from the documents in JSON format. This will allow us to further analyze and utilize the extracted data for various purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import json\n",
    "\n",
    "\n",
    "# Create a DocumentAnalysisClient\n",
    "document_analysis_client = DocumentAnalysisClient(endpoint=os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\"), credential=AzureKeyCredential(os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")))\n",
    "\n",
    "# Function to analyze a document from blob storage\n",
    "def analyze_document(blob_url):\n",
    "    poller = document_analysis_client.begin_analyze_document_from_url(\"prebuilt-document\", blob_url)\n",
    "    result = poller.result()\n",
    "    return result\n",
    "\n",
    "# Retrieve and process documents from blob storage\n",
    "container_client = blob_service_client.get_container_client(os.getenv(\"container_name\"))\n",
    "blob_list = container_client.list_blobs()\n",
    "\n",
    "for blob in blob_list:\n",
    "    blob_url = f\"https://{blob_service_client.account_name}.blob.core.windows.net/{os.getenv(\"container_name\")}/{blob.name}\"\n",
    "    result = analyze_document(blob_url)\n",
    "    \n",
    "    # Convert result to JSON format\n",
    "    result_json = result.to_dict()\n",
    "    print(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Structure the Retrieved Data\n",
    "\n",
    "In this step, we will structure the data retrieved from Azure Document Intelligence. The data will be outputted as a JSON file, and it is our role to process and organize it. Some of the data will be structured into tables, while other data will be formatted as text. This step ensures that the extracted information is organized in a meaningful way for further analysis and usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
