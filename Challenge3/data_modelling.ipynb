{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Modelling: From Retrieval to Upload (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will structure the data retrieved from Azure Document Intelligence (ADI) into the right format to be read by our systems in subsequent steps. \n",
    "\n",
    "The data will be outputted from the ADI as a JSON file, and it is our role to process and organize it. Some of the data will be structured into tables, while other data will be formatted as text. This step ensures that the extracted information is organized in a meaningful way for further analysis and usage.\n",
    "\n",
    "As stated before, we need to make sure that our Function will know how to process:\n",
    "- **Loan Forms:** Extract relevant details such as borrower information, loan amounts, and terms.\n",
    "- **Loan Contract:** Identify and parse key contract elements like clauses, signatures, and dates.\n",
    "- **Pay Stubs:** Retrieve data such as employee details, earnings, deductions, and net pay.\n",
    "\n",
    "Not all customers will have provided all types of content, and during this Challenge we will be only be processing one file. We will combine in the next challenge the capabilities of a trigger, which will, at a time, also process one single document.\n",
    "\n",
    "Due to the nature of this challenge, we will separate this challenge in the 3 different types of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Forms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to get a Loan, is to fill out a form with some basic details, such as customer ID, Full Name, Date Of Birth, etc, therefore, that's where we will start. \n",
    "\n",
    "This particular document combines text and tables, that the ADI capabilities allow you to extract as also separate capabilities.\n",
    "\n",
    "To first start our analysis, let's create a function that will load the documents inside a folder inside a container that is, on its turn, inside our designated Storage Account. In our particular step, inside the folder of the Loan Forms, we will retrieve one Loan Form for us to analyse. \n",
    "\n",
    "We will consequently use this same function to access other folders that will contain other type of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why are we not batch-analysing documents?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def read_json_files_from_blob(folder_path):\n",
    "    # Retrieve the connection string from the environment variables\n",
    "    connection_string = os.getenv('connection_string')\n",
    "\n",
    "    # Ensure the connection string is not None\n",
    "    if connection_string is None:\n",
    "        raise ValueError(\"The connection string environment variable is not set.\")\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(\"bankdetail\")\n",
    "\n",
    "    # List all blobs in the specified folder\n",
    "    blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "\n",
    "    # Filter out JSON files and read their contents\n",
    "    for blob in blob_list:\n",
    "        if blob.name.endswith('.json'):\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            data = json.loads(blob_data)\n",
    "            # print(f\"Contents of {blob.name}:\")\n",
    "            # print(json.dumps(data, indent=2))\n",
    "            # print(\"\\n\")\n",
    "            return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is to call our function and pass the name of our folder as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanform = read_json_files_from_blob(\"loanform\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a function that will process the loan application form data. This function will take the loan application form data as input and return the result of the loan application processing. Our input data is a JSON file that is composed of both text and tables, and we will need to treat both of them seperatly.  \n",
    "\n",
    "The function will perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_structured_tables function processes a list of tables by initializing and populating them with cell content, combining specific rows for tables with 3 rows and 5 columns, and returning the structured tables along with any combined rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    combined_rows = []\n",
    "    \n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        # Combine the last row with the previous one if the table has 5 columns and 3 rows\n",
    "        if row_count == 3 and column_count == 5:\n",
    "            combined_row = [structured_table[1][i] + \" \" + structured_table[2][i] for i in range(column_count)]\n",
    "            structured_table[1] = combined_row\n",
    "            structured_table = structured_table[:2]\n",
    "            combined_rows.append(combined_row)\n",
    "        \n",
    "        # Append the structured table to the list\n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables, combined_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean_form_recognizer_result function processes form recognizer output by extracting text data while ignoring lines containing the word \"table\", retaining only the \"text\" key in each line, and creating structured tables from the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_data = []\n",
    "    table_encountered = False\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                table_encountered = True\n",
    "                continue  # Skip the line if \"table\" is in the text\n",
    "            \n",
    "            if not table_encountered:\n",
    "                # Collect the \"text\" information\n",
    "                text_data.append(line.get(\"text\", \"\"))\n",
    "            \n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables, combined_rows = create_structured_tables(data.get(\"tables\", []))\n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"combined_rows\"] = combined_rows\n",
    "    data[\"text_data\"] = text_data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables_to_dataframes function converts a list of structured tables into a list of pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now retrieved both our table with the structured desired and the text that comes out of our files. However, this function doesn't have the data as structured as we need it to be. \n",
    "\n",
    "As an example, we have by now extracted a key-value pair which keys is \"text\" with the value \"Contact Number: (555) 234-5678\". What we will need to define now is to remove the name of the field, and start composing the key-value pair that would be key \"Contact Number:\" and value \"(555) 234-5678\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "def clean_loan_application_file(text):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    # Extract the category from the first three words\n",
    "    category_match = re.search(r'(\\w+\\s+\\w+\\s+\\w+)', text)\n",
    "    if category_match:\n",
    "        cleaned_data['Category'] = category_match.group(1)\n",
    "    \n",
    "    # Extract Applicant Information\n",
    "    applicant_info = re.search(r'Applicant Information(.*?)Employment and Income Details', text, re.DOTALL)\n",
    "    if applicant_info:\n",
    "        applicant_info_text = applicant_info.group(1)\n",
    "        cleaned_data['Applicant Information'] = {\n",
    "            'Customer ID': re.search(r'Customer ID:\\s*(.*?)Full Name:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Full Name': re.search(r'Full Name:\\s*(.*?)Date of Birth:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Date of Birth': re.search(r'Date of Birth:\\s*(.*?)Social Security Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Social Security Number': re.search(r'Social Security Number:\\s*(.*?)Contact Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Contact Number': re.search(r'Contact Number:\\s*(.*?)Email Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Email Address': re.search(r'Email Address:\\s*(.*?)Physical Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Physical Address': re.search(r'Physical Address:\\s*(.*)', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    # Extract Loan Information\n",
    "    loan_info = re.search(r'Loan Information(.*)', text, re.DOTALL)\n",
    "    if loan_info:\n",
    "        loan_info_text = loan_info.group(1)\n",
    "        cleaned_data['Loan Information'] = {\n",
    "            'Loan Amount Requested': re.search(r'Loan Amount Requested:\\s*\\$?(.*?)Purpose of Loan:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Purpose of Loan': re.search(r'Purpose of Loan:\\s*(.*?)Loan Term Desired:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Loan Term Desired': re.search(r'Loan Term Desired:\\s*(.*)', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to combine extracted tables and text\n",
    "def process_loan_application(data):\n",
    "    # Clean form recognizer result to extract structured tables and text\n",
    "    cleaned_data = clean_form_recognizer_result(data)\n",
    "    \n",
    "    # Convert extracted tables to dataframes\n",
    "    dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"]) \n",
    "    # Combine all table dataframes into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True) \n",
    "    combined_df.columns = combined_df.iloc[0]\n",
    "    combined_df = combined_df[1:]\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    combined_df.rename(columns={\"Contact Number\": \"Employer Contact Number\"}, inplace=True)\n",
    "    combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "    # Clean the extracted text using regex\n",
    "    combined_text = ' '.join(cleaned_data['text_data'])\n",
    "    text_data = clean_loan_application_file(combined_text)\n",
    "\n",
    "    def clean_loan_application(data):\n",
    "    # Extract applicant and loan info\n",
    "        applicant_info = data['Applicant Information']\n",
    "        loan_info = data['Loan Information']\n",
    "        \n",
    "        # Combine keys and values for the two categories\n",
    "        fields = list(applicant_info.keys()) + list(loan_info.keys())\n",
    "        values = list(applicant_info.values()) + list(loan_info.values())\n",
    "        \n",
    "        # Create the 2x10 DataFrame without 'Category'\n",
    "        df = pd.DataFrame({\n",
    "            'Field': fields,\n",
    "            'Value': values\n",
    "        })\n",
    "        \n",
    "        return df.set_index('Field').T\n",
    "\n",
    "    df_cleaned = clean_loan_application(text_data)\n",
    "\n",
    "    # Convert the text data to a DataFrame\n",
    "    text_df = pd.DataFrame(df_cleaned)\n",
    "\n",
    "    # Concatenate the text dataframe with the tables dataframe\n",
    "    final_df = pd.concat([text_df, combined_df], axis=1)\n",
    "\n",
    "    def remove_empty_cells_and_push_up(df):\n",
    "        for column in df.columns:\n",
    "            non_empty_values = df[column].replace('', pd.NA).dropna().values\n",
    "            df[column] = pd.Series(non_empty_values).reindex(df.index, fill_value='')\n",
    "        return df\n",
    "\n",
    "    return remove_empty_cells_and_push_up(final_df)\n",
    "\n",
    "# Process the loan application\n",
    "loanform_structured = process_loan_application(loanform).iloc[1:].reset_index(drop=True)\n",
    "loanform_structured.replace(\"Applicant's Signature:,\", '', regex=True)\n",
    "\n",
    "loanform_structured.to_csv('loanform.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay Stub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of some loan applications, the pay stub is a required document. The pay stub is a document that outlines the details of an employee’s income. It contains the employee’s wages earned, applicable deductions and total gross pay, and net pay for the pay period. A pay stub will provide Contoso bank with crucial information about not only a person’s income and employment stability, which helps assess their ability to repay the loan. It also verifies the applicant’s financial credibility and ensures that their reported income matches their actual earnings.\n",
    "\n",
    "When processing a Pay Stub, we will have similar challenges as we previously did on the Loan Forms. These particular documents combine text and contrary to the previous use case, more than 1 table, Once again, the ADI capabilities allows you to extract these 2 types of entities as also separate capabilities.\n",
    "\n",
    "As we've previously create a the function that will load the documents inside a designated folder, all we have to do now is to retrieve all the information inside the paystub folder, we will retrieve one single Loan Form for us to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "paystub = read_json_files_from_blob(\"paystubs\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_content = []\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                continue  # Keep everything if \"table\" is in the text\n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "            # Collect the text content\n",
    "            text_content.append(line.get(\"text\", \"\"))\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables = create_structured_tables(data.get(\"tables\", []))\n",
    "    \n",
    "    # Concatenate all text content into a single string\n",
    "    plain_text_content = \" \".join(text_content)\n",
    "    \n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"plain_text_content\"] = plain_text_content\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_pay_stub(pay_stub_text):\n",
    "        # Dictionary to store parsed data\n",
    "        parsed_data = {}\n",
    "\n",
    "        # Regular expressions to match the required fields\n",
    "        pay_stub_patterns = {\n",
    "            'Company Name': r'^(.+?) Pay Stub for:',\n",
    "            'Employee Name': r'Pay Stub for: (.+?) Pay Period:',\n",
    "            'Pay Period': r'Pay Period: (.+?) Pay Date:',\n",
    "            'Pay Date': r'Pay Date: (.+?) Employee ID:',\n",
    "            'Employee ': r'Employee ID: (.+?) Employee Information:',\n",
    "            'Customer ID': r'Customer ID: (\\d+)',\n",
    "            'Employee Address': r'Address: (.+?), Social Security',\n",
    "            'Social_Security': r'Social Security Number: (XXX-XX-\\d{4})'\n",
    "        }\n",
    "\n",
    "        # Apply regex patterns and store matches in the dictionary\n",
    "        for key, pattern in pay_stub_patterns.items():\n",
    "            match = re.search(pattern, pay_stub_text)\n",
    "            if match:\n",
    "                parsed_data[key] = match.group(1)\n",
    "        return parsed_data\n",
    "\n",
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables\n",
    "\n",
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "\n",
    "cleaned_data = clean_form_recognizer_result(paystub)\n",
    "dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"])\n",
    "\n",
    "structured_data = {\n",
    "    \"pay stub details\": parse_pay_stub(cleaned_data[\"plain_text_content\"]),\n",
    "}\n",
    "\n",
    "\n",
    "df_list = []\n",
    "\n",
    "def process_dataframe(df):\n",
    "    result = {}\n",
    "    columns = df.columns[1:]  # Ignore the first column\n",
    "    for i in range(1, len(df)):  # Ignore the first row\n",
    "        row_name = df.iloc[i, 0]\n",
    "        result[row_name] = {}\n",
    "        for col in columns:\n",
    "            result[row_name][col] = f\"{row_name} {col}: {df.at[i, col]}\"\n",
    "    return result\n",
    "\n",
    "def rename_json_attributes(json_obj, attribute_titles):\n",
    "    \"\"\"\n",
    "    Rename the keys of a JSON object based on the provided attribute titles.\n",
    "\n",
    "    Parameters:\n",
    "    json_obj (dict): The JSON object to rename.\n",
    "    attribute_titles (dict): A dictionary where keys are the current attribute names and values are the new attribute names.\n",
    "\n",
    "    Returns:\n",
    "    dict: The updated JSON object with renamed keys.\n",
    "    \"\"\"\n",
    "    updated_json = {}\n",
    "    for old_key, new_key in attribute_titles.items():\n",
    "        if old_key in json_obj:\n",
    "            updated_json[new_key] = json_obj[old_key]\n",
    "        else:\n",
    "            updated_json[old_key] = json_obj.get(old_key, None)\n",
    "    return updated_json\n",
    "\n",
    "attribute_titles_earnings = {\n",
    "    \"1\": \"Hours Worked\",\n",
    "    \"2\": \"Rate\",\n",
    "    \"3\": \"Current Earnings\",\n",
    "    \"4\": \"Year-to-Date Earnings\"\n",
    "}\n",
    "\n",
    "attribute_titles_deductions = {\n",
    "    \"1\": \"Current Amount\",\n",
    "    \"2\": \"Year-to-Date Amount\"\n",
    "}\n",
    "\n",
    "# Process the earnings and deductions DataFrames\n",
    "earnings_dict = process_dataframe(dataframes[0])\n",
    "deductions_dict = process_dataframe(dataframes[1])\n",
    "# Append the processed DataFrames to the JSON structure\n",
    "structured_data[\"earnings\"] = earnings_dict\n",
    "structured_data[\"deductions\"] = deductions_dict\n",
    "\n",
    "def clean_pay_stub_section(data):\n",
    "    # Check for 'deductions' and 'earnings' in the data\n",
    "    for section in ['deductions', 'earnings']:\n",
    "        if section in data:\n",
    "            for key, values in data[section].items():\n",
    "                # For each entry, clean up the values by removing everything before the colon\n",
    "                for subkey in values:\n",
    "                    # Split the string by colon and take the second part, stripping whitespace\n",
    "                    values[subkey] = values[subkey].split(\":\")[1].strip()\n",
    "    return data\n",
    "\n",
    "structured_data = clean_pay_stub_section(structured_data)\n",
    "\n",
    "\n",
    "def update_attribute_keys(data, section, key_mapping):\n",
    "    # Ensure the section exists in the data (either \"earnings\" or \"deductions\")\n",
    "    if section in data:\n",
    "        # Iterate over each type within the earnings or deductions section\n",
    "        for entry_type, attributes in data[section].items():\n",
    "            # Create a new dictionary to store the updated attributes\n",
    "            updated_attributes = {}\n",
    "            \n",
    "            # Loop through each attribute in that entry (e.g. 1, 2, 3)\n",
    "            for old_key, value in attributes.items():\n",
    "                # Map the old key (which is an integer) to the new descriptive key using key_mapping\n",
    "                if str(old_key) in key_mapping:  # Convert old_key to string to match the mapping\n",
    "                    new_key = key_mapping[str(old_key)]\n",
    "                else:\n",
    "                    new_key = old_key  # If no mapping is found, retain the old key\n",
    "                \n",
    "                # Update the dictionary with the new key\n",
    "                updated_attributes[new_key] = value\n",
    "\n",
    "            # Replace the old attributes with the updated attributes in the data\n",
    "            data[section][entry_type] = updated_attributes\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "paystub_final = update_attribute_keys(structured_data, \"earnings\", attribute_titles_earnings)\n",
    "paystub_final = structured_data = update_attribute_keys(structured_data, \"deductions\", attribute_titles_deductions)\n",
    "\n",
    "\n",
    "# Save the updated JSON structure back to the file\n",
    "with open('paystub_details.json', 'w') as json_file:\n",
    "    json.dump(paystub_final, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we get to the last part of our logical set of documents on a loan application process: the final loan agreement contract has been created and signed. A loan agreement contract is a legally binding document between a lender and a borrower that outlines the terms and conditions of a loan. This contract specifies the loan amount, interest rate, repayment schedule, and any other obligations or rights of both parties. It is crucial as it provides clarity and protection for both the lender and the borrower, ensuring that both parties understand their responsibilities and the consequences of default. Additionally, it serves as a legal record that can be referenced in case of disputes, helping to prevent misunderstandings and enforce the agreed-upon terms.\n",
    "\n",
    "The format of a loan agreement is, on its core, a text document that will not have a fixed structure. We should expect just as an input text document and therefore retrieve it as such. \n",
    "\n",
    "As we did in the previous steps, let's call the function that will retrieve the information inside the loanagreements folder, retrieving, once again, one single Loan Agreement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loan Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanagreement = read_json_files_from_blob(\"loanagreements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LOAN AGREEMENT This Loan Agreement (\\\"Agreement\\\") is made and entered into on August 1, 2024, by and between: \\u00b7 Lender: Horizon Bank Address: 123 Finance Avenue, Madison, WI 53703 Contact Number: (555) 123-4567 Email: lending@horizonbank.com \\u00b7 Borrower: Jane Elizabeth Smith Customer ID: 100002 Address: 456 Oak Avenue, Unit 10, Madison, WI 53703 Contact Number: (555) 234-5678 Email: jane.smith90@example.com 1. Loan Amount and Purpose 1.1 Loan Amount: The Lender agrees to loan the Borrower the principal sum of $30,000.00 (thirty thousand dollars), referred to as the \\\"Loan.\\\" 1.2 Purpose of Loan: The Loan shall be used exclusively for the purchase of a vehicle, specifically a 2022 Toyota Camry. 2. Interest Rate 2.1 Interest Rate: The Loan shall bear interest at an annual fixed rate of 5.5%. 2.2 Accrual: Interest shall begin to accrue on the Loan from the date the funds are disbursed to the Borrower. 3. Loan Term 3.1 Term: The term of this Loan shall be 5 years (60 months), commencing on August 1, 2024, and ending on August 1, 2029. 4. Repayment 4.1 Monthly Payments: The Borrower agrees to make monthly payments of $573.99, which includes both principal and interest, starting on September 1, 2024, and continuing on the first day of each month thereafter until the Loan is fully repaid. 4.2 Prepayment: The Borrower may prepay the Loan in whole or in part at any time without penalty. Any prepayment will be applied first to accrued interest, then to the principal. 4.3 Late Payments: If any payment is not received within 10 days of the due date, the Borrower shall be charged a late fee of $25.00. 5. Security 5.1 Collateral: The Loan is secured by the vehicle purchased with the Loan proceeds, specifically a 2022 Toyota Camry. The Borrower grants the Lender a security interest in the vehicle. 5.2 Title and Insurance: The Borrower agrees to maintain full insurance coverage on the vehicle and to name the Lender as the loss payee on the insurance policy. The Borrower shall provide proof of insurance to the Lender within 30 days of the Loan disbursement. 6. Default 6.1 Events of Default: The following events shall constitute an event of default under this Agreement: . Failure to make any payment when due. . Breach of any term or condition of this Agreement. . Bankruptcy or insolvency of the Borrower. . Sale or transfer of the collateral without the Lender's consent. 6.2 Remedies: Upon the occurrence of an event of default, the Lender may declare the entire unpaid principal and accrued interest immediately due and payable. The Lender may also repossess the collateral or pursue any other remedy available at law or in equity. 7. Miscellaneous 7.1 Governing Law: This Agreement shall be governed by and construed in accordance with the laws of the State of Wisconsin. 7.2 Amendments: No amendment or modification of this Agreement shall be effective unless in writing and signed by both parties. 7.3 Severability: If any provision of this Agreement is found to be invalid or unenforceable, the remaining provisions shall continue in full force and effect. 7.4 Entire Agreement: This Agreement constitutes the entire understanding between the parties regarding the Loan and supersedes all prior discussions, agreements, or understandings of any kind. 8. Signatures IN WITNESS WHEREOF, the parties have executed this Loan Agreement as of the day and year first above written. Lender: Horizon Bank By: Name: Michael Grant Title: Loan Officer Date: August 1, 2024 Borrower: By: Jane Elizabeth Smith Customer ID: 100002 Date: August 1, 2024\"\n",
      "Customer ID: 100002\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_data(json_data):\n",
    "    # Extract relevant text content from the JSON\n",
    "    content = []\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    paragraphs = json_data.get(\"paragraphs\", [])\n",
    "    for paragraph in paragraphs:\n",
    "        content.append(paragraph.get(\"text\", \"\").strip())\n",
    "\n",
    "    # Extract text from pages and lines\n",
    "    pages = json_data.get(\"pages\", [])\n",
    "    for page in pages:\n",
    "        for line in page.get(\"lines\", []):\n",
    "            content.append(line.get(\"text\", \"\").strip())\n",
    "\n",
    "    # Join all text content into a single string with spaces between components\n",
    "    plain_text_content = \" \".join(content)\n",
    "\n",
    "    # Extract Customer ID using regex\n",
    "    pattern = r\"Customer ID:\\s*(\\d+)\"\n",
    "    match = re.search(pattern, plain_text_content)\n",
    "    customer_id = match.group(1) if match else None\n",
    "    return plain_text_content, customer_id\n",
    "\n",
    "# Clean the JSON data and extract Customer ID\n",
    "loanagreement_structured, customer_id = clean_json_data(loanagreement)\n",
    "\n",
    "# Print the cleaned data and Customer ID\n",
    "print(json.dumps(loanagreement_structured, indent=2))\n",
    "print(f\"Customer ID: {customer_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Architecturing: From Retrieval to Upload (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Cosmos - Loan Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the database or container: (BadRequest) Message: {\"Errors\":[\"The partition key component definition path '\\/Customer ID' could not be accepted, failed near position '10'. Partition key paths must contain only valid characters and not contain a trailing slash or wildcard character.\"]}\n",
      "ActivityId: 3b2de5fa-aee4-4a87-bfb7-2ebcb3b9b20e, Request URI: /apps/cef05ddd-1219-4cd3-b3f4-ff8fbe366a96/services/99db3e94-a0c1-43ec-89f9-86b2578f6f45/partitions/e9c36b19-24ea-42a3-bd30-571c81ea02e5/replicas/133691424048599213p, RequestStats: , SDK: Microsoft.Azure.Documents.Common/2.14.0\n",
      "Code: BadRequest\n",
      "Message: Message: {\"Errors\":[\"The partition key component definition path '\\/Customer ID' could not be accepted, failed near position '10'. Partition key paths must contain only valid characters and not contain a trailing slash or wildcard character.\"]}\n",
      "ActivityId: 3b2de5fa-aee4-4a87-bfb7-2ebcb3b9b20e, Request URI: /apps/cef05ddd-1219-4cd3-b3f4-ff8fbe366a96/services/99db3e94-a0c1-43ec-89f9-86b2578f6f45/partitions/e9c36b19-24ea-42a3-bd30-571c81ea02e5/replicas/133691424048599213p, RequestStats: , SDK: Microsoft.Azure.Documents.Common/2.14.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"COSMOS_DB_ENDPOINT\")\n",
    "key = os.getenv(\"COSMOS_DB_KEY\")\n",
    "database_name = os.getenv(\"COSMOS_DB_DATABASE_NAME\")\n",
    "container_name = os.getenv(\"COSMOS_DB_CONTAINER_NAME\")\n",
    "file_name = \"data2\"\n",
    "\n",
    "def upload_dataframe_to_cosmos_db(df, file_name, endpoint, key, database_name, container_name, partition_key_column):\n",
    "    # Check if DataFrame is empty\n",
    "    if df.empty:\n",
    "        print(\"The DataFrame is empty. No data to upload.\")\n",
    "        return\n",
    "    \n",
    "    # Check if partition key column exists in DataFrame\n",
    "    if partition_key_column not in df.columns:\n",
    "        print(f\"The partition key column '{partition_key_column}' does not exist in the DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the Cosmos client\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    \n",
    "    try:\n",
    "        # Create or get the database\n",
    "        database = client.create_database_if_not_exists(id=database_name)\n",
    "        \n",
    "        # Create or get the container\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path=f\"/{partition_key_column}\"),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while creating the database or container: {e.message}\")\n",
    "        return\n",
    "    \n",
    "    # Convert DataFrame to JSON string\n",
    "    json_content = df.to_json(orient='records')\n",
    "    \n",
    "    # Parse the JSON string\n",
    "    json_data = json.loads(json_content)\n",
    "    \n",
    "    # Create documents with the JSON content and partition key\n",
    "    for record in json_data:\n",
    "        document = {\n",
    "            'id': str(uuid.uuid4()),  # Generate a unique ID for each document\n",
    "            'content': record,\n",
    "            partition_key_column: record[partition_key_column]\n",
    "        }\n",
    "        \n",
    "        # Upload the document to the container\n",
    "        try:\n",
    "            container.create_item(body=document)\n",
    "            print(f\"JSON content uploaded successfully with ID '{document['id']}' in Cosmos DB.\")\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"An error occurred while uploading the document: {e.message}\")\n",
    "\n",
    "# Example usage\n",
    "upload_dataframe_to_cosmos_db(loanform_structured, \"loanform1\", endpoint, key, database_name, container_name, loanform_structured.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Cosmos - Loan Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the database or container: (BadRequest) Message: {\"Errors\":[\"The input name 'null' is invalid. Ensure to provide a unique non-empty string less than '255' characters.\",\"The request payload is invalid. Ensure to provide a valid request payload.\"]}\n",
      "ActivityId: c40737f9-0b68-4a4e-8c75-34d942f2465f, Request URI: /apps/cef05ddd-1219-4cd3-b3f4-ff8fbe366a96/services/99db3e94-a0c1-43ec-89f9-86b2578f6f45/partitions/e9c36b19-24ea-42a3-bd30-571c81ea02e5/replicas/133691424048599213p, RequestStats: \n",
      "RequestStartTime: 2024-09-08T12:52:08.6721927Z, RequestEndTime: 2024-09-08T12:52:08.6738012Z,  Number of regions attempted:1\n",
      "{\"systemHistory\":[{\"dateUtc\":\"2024-09-08T12:51:12.5417569Z\",\"cpu\":0.049,\"memory\":660240992.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0742,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":219},{\"dateUtc\":\"2024-09-08T12:51:22.5517950Z\",\"cpu\":0.039,\"memory\":660224896.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0697,\"availableThreads\":32764,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":219},{\"dateUtc\":\"2024-09-08T12:51:32.5617558Z\",\"cpu\":0.185,\"memory\":660235068.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0433,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":192},{\"dateUtc\":\"2024-09-08T12:51:42.5717882Z\",\"cpu\":0.054,\"memory\":660238972.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0257,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":192},{\"dateUtc\":\"2024-09-08T12:51:52.5817754Z\",\"cpu\":0.034,\"memory\":660217580.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0509,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":192},{\"dateUtc\":\"2024-09-08T12:52:02.5918194Z\",\"cpu\":0.193,\"memory\":660208576.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0685,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":174}]}\n",
      "RequestStart: 2024-09-08T12:52:08.6722267Z; ResponseTime: 2024-09-08T12:52:08.6738012Z; StoreResult: StorePhysicalAddress: rntbd://10.0.1.10:11300/apps/cef05ddd-1219-4cd3-b3f4-ff8fbe366a96/services/99db3e94-a0c1-43ec-89f9-86b2578f6f45/partitions/e9c36b19-24ea-42a3-bd30-571c81ea02e5/replicas/133691424048599213p, LSN: 14, GlobalCommittedLsn: 14, PartitionKeyRangeId: , IsValid: True, StatusCode: 400, SubStatusCode: 0, RequestCharge: 1.24, ItemLSN: -1, SessionToken: -1#14, UsingLocalLSN: False, TransportException: null, BELatencyMs: 0.423, ActivityId: c40737f9-0b68-4a4e-8c75-34d942f2465f, RetryAfterInMs: , ReplicaHealthStatuses: [(port: 11300 | status: Connected | lkt: 9/7/2024 6:18:12 AM)], TransportRequestTimeline: {\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-08T12:52:08.6722435Z\", \"durationInMs\": 0.0131},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-08T12:52:08.6722566Z\", \"durationInMs\": 0.0104},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-08T12:52:08.6722670Z\", \"durationInMs\": 0.0639},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-08T12:52:08.6723309Z\", \"durationInMs\": 1.299},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-08T12:52:08.6736299Z\", \"durationInMs\": 0.0599},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-08T12:52:08.6736898Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"False\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-08T12:51:54.8599861Z\",\"lastSend\":\"2024-09-08T12:51:54.8599980Z\",\"lastReceive\":\"2024-09-08T12:51:54.8608827Z\"},\"requestSizeInBytes\":528,\"requestBodySizeInBytes\":15,\"responseMetadataSizeInBytes\":171,\"responseBodySizeInBytes\":197};\n",
      " ResourceType: Database, OperationType: Create\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.14.0\n",
      "Code: BadRequest\n",
      "Message: Message: {\"Errors\":[\"The input name 'null' is invalid. Ensure to provide a unique non-empty string less than '255' characters.\",\"The request payload is invalid. Ensure to provide a valid request payload.\"]}\n",
      "ActivityId: c40737f9-0b68-4a4e-8c75-34d942f2465f, Request URI: /apps/cef05ddd-1219-4cd3-b3f4-ff8fbe366a96/services/99db3e94-a0c1-43ec-89f9-86b2578f6f45/partitions/e9c36b19-24ea-42a3-bd30-571c81ea02e5/replicas/133691424048599213p, RequestStats: \n",
      "RequestStartTime: 2024-09-08T12:52:08.6721927Z, RequestEndTime: 2024-09-08T12:52:08.6738012Z,  Number of regions attempted:1\n",
      "{\"systemHistory\":[{\"dateUtc\":\"2024-09-08T12:51:12.5417569Z\",\"cpu\":0.049,\"memory\":660240992.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0742,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":219},{\"dateUtc\":\"2024-09-08T12:51:22.5517950Z\",\"cpu\":0.039,\"memory\":660224896.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0697,\"availableThreads\":32764,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":219},{\"dateUtc\":\"2024-09-08T12:51:32.5617558Z\",\"cpu\":0.185,\"memory\":660235068.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0433,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":192},{\"dateUtc\":\"2024-09-08T12:51:42.5717882Z\",\"cpu\":0.054,\"memory\":660238972.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0257,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":192},{\"dateUtc\":\"2024-09-08T12:51:52.5817754Z\",\"cpu\":0.034,\"memory\":660217580.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0509,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":192},{\"dateUtc\":\"2024-09-08T12:52:02.5918194Z\",\"cpu\":0.193,\"memory\":660208576.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0685,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":174}]}\n",
      "RequestStart: 2024-09-08T12:52:08.6722267Z; ResponseTime: 2024-09-08T12:52:08.6738012Z; StoreResult: StorePhysicalAddress: rntbd://10.0.1.10:11300/apps/cef05ddd-1219-4cd3-b3f4-ff8fbe366a96/services/99db3e94-a0c1-43ec-89f9-86b2578f6f45/partitions/e9c36b19-24ea-42a3-bd30-571c81ea02e5/replicas/133691424048599213p, LSN: 14, GlobalCommittedLsn: 14, PartitionKeyRangeId: , IsValid: True, StatusCode: 400, SubStatusCode: 0, RequestCharge: 1.24, ItemLSN: -1, SessionToken: -1#14, UsingLocalLSN: False, TransportException: null, BELatencyMs: 0.423, ActivityId: c40737f9-0b68-4a4e-8c75-34d942f2465f, RetryAfterInMs: , ReplicaHealthStatuses: [(port: 11300 | status: Connected | lkt: 9/7/2024 6:18:12 AM)], TransportRequestTimeline: {\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-08T12:52:08.6722435Z\", \"durationInMs\": 0.0131},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-08T12:52:08.6722566Z\", \"durationInMs\": 0.0104},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-08T12:52:08.6722670Z\", \"durationInMs\": 0.0639},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-08T12:52:08.6723309Z\", \"durationInMs\": 1.299},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-08T12:52:08.6736299Z\", \"durationInMs\": 0.0599},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-08T12:52:08.6736898Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"False\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-08T12:51:54.8599861Z\",\"lastSend\":\"2024-09-08T12:51:54.8599980Z\",\"lastReceive\":\"2024-09-08T12:51:54.8608827Z\"},\"requestSizeInBytes\":528,\"requestBodySizeInBytes\":15,\"responseMetadataSizeInBytes\":171,\"responseBodySizeInBytes\":197};\n",
      " ResourceType: Database, OperationType: Create\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.14.0\n"
     ]
    }
   ],
   "source": [
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Cosmos DB connection details from environment variables\n",
    "endpoint = os.getenv(\"COSMOS_DB_ENDPOINT\")\n",
    "key = os.getenv(\"COSMOS_DB_KEY\")\n",
    "database_name = os.getenv(\"COSMOS_DB_DATABASE\")\n",
    "container_name = os.getenv(\"COSMOS_DB_CONTAINER\")\n",
    "\n",
    "def upload_text_to_cosmos_db(text_content, partition_key_value):\n",
    "    # Check if the text is empty\n",
    "    if not text_content:\n",
    "        print(\"The text content is empty. No data to upload.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the Cosmos client\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    \n",
    "    try:\n",
    "        # Create or get the database\n",
    "        database = client.create_database_if_not_exists(id=database_name)\n",
    "        \n",
    "        # Create or get the container\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path=f\"/{partition_key_value}\"),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while creating the database or container: {e.message}\")\n",
    "        return\n",
    "    \n",
    "    # Create a document with the text content and partition key\n",
    "    document = {\n",
    "        'id': str(customer_id),  # Generate a unique ID for the document\n",
    "        'content': text_content,  # Store the plain text as 'content'\n",
    "        partition_key_value: partition_key_value  # Use the partition key value\n",
    "    }\n",
    "    \n",
    "    # Upload the document to the container\n",
    "    try:\n",
    "        container.create_item(body=document)\n",
    "        print(f\"Text content uploaded successfully with ID '{document['id']}' in Cosmos DB.\")\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while uploading the document: {e.message}\")\n",
    "\n",
    "# Pass in the plain text and a partition key value\n",
    "upload_text_to_cosmos_db(loanagreement_structured, customer_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
