{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Modelling: From Retrieval to Upload (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will structure the data retrieved from Azure Document Intelligence (ADI) into the right format to be read by our systems in subsequent steps. \n",
    "\n",
    "The data will be outputted from the ADI as a JSON file, and it is our role to process and organize it. Some of the data will be structured into tables, while other data will be formatted as text. This step ensures that the extracted information is organized in a meaningful way for further analysis and usage.\n",
    "\n",
    "As stated before, we need to make sure that our Function will know how to process:\n",
    "- **Loan Forms:** Extract relevant details such as borrower information, loan amounts, and terms.\n",
    "- **Loan Contract:** Identify and parse key contract elements like clauses, signatures, and dates.\n",
    "- **Pay Stubs:** Retrieve data such as employee details, earnings, deductions, and net pay.\n",
    "\n",
    "Not all customers will have provided all types of content, and during this Challenge we will be only be processing one file. We will combine in the next challenge the capabilities of a trigger, which will, at a time, also process one single document.\n",
    "\n",
    "Due to the nature of this challenge, we will separate this challenge in the 3 different types of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Forms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to get a Loan, is to fill out a form with some basic details, such as customer ID, Full Name, Date Of Birth, etc, therefore, that's where we will start. \n",
    "\n",
    "This particular document combines text and tables, that the ADI capabilities allow you to extract as also separate capabilities.\n",
    "\n",
    "To first start our analysis, let's create a function that will load the documents inside a folder inside a container that is, on its turn, inside our designated Storage Account. In our particular step, inside the folder of the Loan Forms, we will retrieve one Loan Form for us to analyse. \n",
    "\n",
    "We will consequently use this same function to access other folders that will contain other type of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why are we not batch-analysing documents?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def read_json_files_from_blob(folder_path):\n",
    "    # Retrieve the connection string from the environment variables\n",
    "    connection_string = os.getenv('connection_string')\n",
    "\n",
    "    # Ensure the connection string is not None\n",
    "    if connection_string is None:\n",
    "        raise ValueError(\"The connection string environment variable is not set.\")\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(\"bankdetail\")\n",
    "\n",
    "    # List all blobs in the specified folder\n",
    "    blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "\n",
    "    # Filter out JSON files and read their contents\n",
    "    for blob in blob_list:\n",
    "        if blob.name.endswith('.json'):\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            data = json.loads(blob_data)\n",
    "            # print(f\"Contents of {blob.name}:\")\n",
    "            # print(json.dumps(data, indent=2))\n",
    "            # print(\"\\n\")\n",
    "            return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is to call our function and pass the name of our folder as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanform = read_json_files_from_blob(\"loanform\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a function that will process the loan application form data. This function will take the loan application form data as input and return the result of the loan application processing. Our input data is a JSON file that is composed of both text and tables, and we will need to treat both of them seperatly.  \n",
    "\n",
    "The function will perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_structured_tables function processes a list of tables by initializing and populating them with cell content, combining specific rows for tables with 3 rows and 5 columns, and returning the structured tables along with any combined rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    combined_rows = []\n",
    "    \n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        # Combine the last row with the previous one if the table has 5 columns and 3 rows\n",
    "        if row_count == 3 and column_count == 5:\n",
    "            combined_row = [structured_table[1][i] + \" \" + structured_table[2][i] for i in range(column_count)]\n",
    "            structured_table[1] = combined_row\n",
    "            structured_table = structured_table[:2]\n",
    "            combined_rows.append(combined_row)\n",
    "        \n",
    "        # Append the structured table to the list\n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables, combined_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean_form_recognizer_result function processes form recognizer output by extracting text data while ignoring lines containing the word \"table\", retaining only the \"text\" key in each line, and creating structured tables from the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_data = []\n",
    "    table_encountered = False\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                table_encountered = True\n",
    "                continue  # Skip the line if \"table\" is in the text\n",
    "            \n",
    "            if not table_encountered:\n",
    "                # Collect the \"text\" information\n",
    "                text_data.append(line.get(\"text\", \"\"))\n",
    "            \n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables, combined_rows = create_structured_tables(data.get(\"tables\", []))\n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"combined_rows\"] = combined_rows\n",
    "    data[\"text_data\"] = text_data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables_to_dataframes function converts a list of structured tables into a list of pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now retrieved both our table with the structured desired and the text that comes out of our files. However, this function doesn't have the data as structured as we need it to be. \n",
    "\n",
    "As an example, we have by now extracted a key-value pair which keys is \"text\" with the value \"Contact Number: (555) 234-5678\". What we will need to define now is to remove the name of the field, and start composing the key-value pair that would be key \"Contact Number:\" and value \"(555) 234-5678\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted to JSON object:\n",
      "[\n",
      "    {\n",
      "        \"id\": \"100001\",\n",
      "        \"Full Name\": \"Jane Elizabeth Smith\",\n",
      "        \"Date of Birth\": \"08/22/1990\",\n",
      "        \"Social Security Number\": \"987-65-4321\",\n",
      "        \"Contact Number\": \"(555) 234-5678\",\n",
      "        \"Email Address\": \"jane.smith90@example.com\",\n",
      "        \"Physical Address\": \"456 Oak Avenue Unit 10 Madison WI 53703\",\n",
      "        \"Loan Amount Requested\": \"30000\",\n",
      "        \"Purpose of Loan\": \"Vehicle Purchase\",\n",
      "        \"Loan Term Desired\": \"5 years Applicant's Signature:\",\n",
      "        \"Employer Name\": \"Horizon Retailers\",\n",
      "        \"Position\": \"Store Manager \",\n",
      "        \"Employment Duration\": \"3 years \",\n",
      "        \"Monthly Income\": \"$4583.33 \",\n",
      "        \"Employer Contact Number\": \"(555) 789- 2345\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "def clean_loan_application_file(text):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    # Extract the category from the first three words\n",
    "    category_match = re.search(r'(\\w+\\s+\\w+\\s+\\w+)', text)\n",
    "    if category_match:\n",
    "        cleaned_data['Category'] = category_match.group(1)\n",
    "    \n",
    "    # Extract Applicant Information\n",
    "    applicant_info = re.search(r'Applicant Information(.*?)Employment and Income Details', text, re.DOTALL)\n",
    "    if applicant_info:\n",
    "        applicant_info_text = applicant_info.group(1)\n",
    "        cleaned_data['Applicant Information'] = {\n",
    "            'id': re.search(r'Customer ID:\\s*(.*?)Full Name:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Full Name': re.search(r'Full Name:\\s*(.*?)Date of Birth:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Date of Birth': re.search(r'Date of Birth:\\s*(.*?)Social Security Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Social Security Number': re.search(r'Social Security Number:\\s*(.*?)Contact Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Contact Number': re.search(r'Contact Number:\\s*(.*?)Email Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Email Address': re.search(r'Email Address:\\s*(.*?)Physical Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Physical Address': re.search(r'Physical Address:\\s*(.*)', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    # Extract Loan Information\n",
    "    loan_info = re.search(r'Loan Information(.*)', text, re.DOTALL)\n",
    "    if loan_info:\n",
    "        loan_info_text = loan_info.group(1)\n",
    "        cleaned_data['Loan Information'] = {\n",
    "            'Loan Amount Requested': re.search(r'Loan Amount Requested:\\s*\\$?(.*?)Purpose of Loan:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Purpose of Loan': re.search(r'Purpose of Loan:\\s*(.*?)Loan Term Desired:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Loan Term Desired': re.search(r'Loan Term Desired:\\s*(.*)', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to combine extracted tables and text\n",
    "def process_loan_application(data):\n",
    "    # Clean form recognizer result to extract structured tables and text\n",
    "    cleaned_data = clean_form_recognizer_result(data)\n",
    "    \n",
    "    # Convert extracted tables to dataframes\n",
    "    dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"]) \n",
    "    # Combine all table dataframes into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True) \n",
    "    combined_df.columns = combined_df.iloc[0]\n",
    "    combined_df = combined_df[1:]\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    combined_df.rename(columns={\"Contact Number\": \"Employer Contact Number\"}, inplace=True)\n",
    "    combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "    # Clean the extracted text using regex\n",
    "    combined_text = ' '.join(cleaned_data['text_data'])\n",
    "    text_data = clean_loan_application_file(combined_text)\n",
    "\n",
    "    def clean_loan_application(data):\n",
    "    # Extract applicant and loan info\n",
    "        applicant_info = data['Applicant Information']\n",
    "        loan_info = data['Loan Information']\n",
    "        \n",
    "        # Combine keys and values for the two categories\n",
    "        fields = list(applicant_info.keys()) + list(loan_info.keys())\n",
    "        values = list(applicant_info.values()) + list(loan_info.values())\n",
    "        \n",
    "        # Create the 2x10 DataFrame without 'Category'\n",
    "        df = pd.DataFrame({\n",
    "            'Field': fields,\n",
    "            'Value': values\n",
    "        })\n",
    "        \n",
    "        return df.set_index('Field').T\n",
    "\n",
    "    df_cleaned = clean_loan_application(text_data)\n",
    "\n",
    "    # Convert the text data to a DataFrame\n",
    "    text_df = pd.DataFrame(df_cleaned)\n",
    "\n",
    "    # Concatenate the text dataframe with the tables dataframe\n",
    "    final_df = pd.concat([text_df, combined_df], axis=1)\n",
    "\n",
    "    def remove_empty_cells_and_push_up(df):\n",
    "        for column in df.columns:\n",
    "            non_empty_values = df[column].replace('', pd.NA).dropna().values\n",
    "            df[column] = pd.Series(non_empty_values).reindex(df.index, fill_value='')\n",
    "        return df\n",
    "    return remove_empty_cells_and_push_up(final_df)\n",
    "\n",
    "# Process the loan application\n",
    "loanform_structured = process_loan_application(loanform).iloc[1:].reset_index(drop=True)\n",
    "loanform_structured.replace(\"Applicant's Signature:,\", '', regex=True, inplace=True)\n",
    "loanform_structured.replace(\"\\,\", '', regex=True,  inplace=True)\n",
    "\n",
    "# Convert DataFrame to JSON\n",
    "json_loanform = loanform_structured.to_json(orient=\"records\")\n",
    "\n",
    "# Convert JSON string to a Python dictionary\n",
    "data = json.loads(json_loanform)\n",
    "\n",
    "#Step 1: Remove unwanted characters (if necessary)\n",
    "cleaned_json = json_loanform.strip()\n",
    "\n",
    "# Step 2: Replace escaped characters\n",
    "cleaned_json = cleaned_json.replace('\\n', '').replace('\\t', '').replace('\\r', '')\n",
    "\n",
    "# Step 3: Load the cleaned string into a JSON object\n",
    "try:\n",
    "    json_data = json.loads(cleaned_json)\n",
    "    print(\"Successfully converted to JSON object:\")\n",
    "    print(json.dumps(json_data, indent=4))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    print(\"Cleaned JSON string:\")\n",
    "    print(cleaned_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay Stub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of some loan applications, the pay stub is a required document. The pay stub is a document that outlines the details of an employee’s income. It contains the employee’s wages earned, applicable deductions and total gross pay, and net pay for the pay period. A pay stub will provide Contoso bank with crucial information about not only a person’s income and employment stability, which helps assess their ability to repay the loan. It also verifies the applicant’s financial credibility and ensures that their reported income matches their actual earnings.\n",
    "\n",
    "When processing a Pay Stub, we will have similar challenges as we previously did on the Loan Forms. These particular documents combine text and contrary to the previous use case, more than 1 table, Once again, the ADI capabilities allows you to extract these 2 types of entities as also separate capabilities.\n",
    "\n",
    "As we've previously create a the function that will load the documents inside a designated folder, all we have to do now is to retrieve all the information inside the paystub folder, we will retrieve one single Loan Form for us to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "paystub = read_json_files_from_blob(\"paystubs\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_content = []\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                continue  # Keep everything if \"table\" is in the text\n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "            # Collect the text content\n",
    "            text_content.append(line.get(\"text\", \"\"))\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables = create_structured_tables(data.get(\"tables\", []))\n",
    "    \n",
    "    # Concatenate all text content into a single string\n",
    "    plain_text_content = \" \".join(text_content)\n",
    "    \n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"plain_text_content\"] = plain_text_content\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_pay_stub(pay_stub_text):\n",
    "        # Dictionary to store parsed data\n",
    "        parsed_data = {}\n",
    "\n",
    "        # Regular expressions to match the required fields\n",
    "        pay_stub_patterns = {\n",
    "            'id': r'Customer ID: (\\d+)',\n",
    "            'Company Name': r'^(.+?) Pay Stub for:',\n",
    "            'Employee Name': r'Pay Stub for: (.+?) Pay Period:',\n",
    "            'Pay Period': r'Pay Period: (.+?) Pay Date:',\n",
    "            'Pay Date': r'Pay Date: (.+?) Employee ID:',\n",
    "            'Employee ': r'Employee ID: (.+?) Employee Information:',\n",
    "            'Employee Address': r'Address: (.+?), Social Security',\n",
    "            'Social_Security': r'Social Security Number: (XXX-XX-\\d{4})'\n",
    "        }\n",
    "\n",
    "        # Apply regex patterns and store matches in the dictionary\n",
    "        for key, pattern in pay_stub_patterns.items():\n",
    "            match = re.search(pattern, pay_stub_text)\n",
    "            if match:\n",
    "                parsed_data[key] = match.group(1)\n",
    "        return parsed_data\n",
    "\n",
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables\n",
    "\n",
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "\n",
    "cleaned_data = clean_form_recognizer_result(paystub)\n",
    "dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"])\n",
    "\n",
    "structured_data = {\n",
    "    \"pay stub details\": parse_pay_stub(cleaned_data[\"plain_text_content\"]),\n",
    "}\n",
    "\n",
    "\n",
    "df_list = []\n",
    "\n",
    "def process_dataframe(df):\n",
    "    result = {}\n",
    "    columns = df.columns[1:]  # Ignore the first column\n",
    "    for i in range(1, len(df)):  # Ignore the first row\n",
    "        row_name = df.iloc[i, 0]\n",
    "        result[row_name] = {}\n",
    "        for col in columns:\n",
    "            result[row_name][col] = f\"{row_name} {col}: {df.at[i, col]}\"\n",
    "    return result\n",
    "\n",
    "def rename_json_attributes(json_obj, attribute_titles):\n",
    "    \"\"\"\n",
    "    Rename the keys of a JSON object based on the provided attribute titles.\n",
    "\n",
    "    Parameters:\n",
    "    json_obj (dict): The JSON object to rename.\n",
    "    attribute_titles (dict): A dictionary where keys are the current attribute names and values are the new attribute names.\n",
    "\n",
    "    Returns:\n",
    "    dict: The updated JSON object with renamed keys.\n",
    "    \"\"\"\n",
    "    updated_json = {}\n",
    "    for old_key, new_key in attribute_titles.items():\n",
    "        if old_key in json_obj:\n",
    "            updated_json[new_key] = json_obj[old_key]\n",
    "        else:\n",
    "            updated_json[old_key] = json_obj.get(old_key, None)\n",
    "    return updated_json\n",
    "\n",
    "attribute_titles_earnings = {\n",
    "    \"1\": \"Hours Worked\",\n",
    "    \"2\": \"Rate\",\n",
    "    \"3\": \"Current Earnings\",\n",
    "    \"4\": \"Year-to-Date Earnings\"\n",
    "}\n",
    "\n",
    "attribute_titles_deductions = {\n",
    "    \"1\": \"Current Amount\",\n",
    "    \"2\": \"Year-to-Date Amount\"\n",
    "}\n",
    "\n",
    "# Process the earnings and deductions DataFrames\n",
    "earnings_dict = process_dataframe(dataframes[0])\n",
    "deductions_dict = process_dataframe(dataframes[1])\n",
    "# Append the processed DataFrames to the JSON structure\n",
    "structured_data[\"earnings\"] = earnings_dict\n",
    "structured_data[\"deductions\"] = deductions_dict\n",
    "\n",
    "def clean_pay_stub_section(data):\n",
    "    # Check for 'deductions' and 'earnings' in the data\n",
    "    for section in ['deductions', 'earnings']:\n",
    "        if section in data:\n",
    "            for key, values in data[section].items():\n",
    "                # For each entry, clean up the values by removing everything before the colon\n",
    "                for subkey in values:\n",
    "                    # Split the string by colon and take the second part, stripping whitespace\n",
    "                    values[subkey] = values[subkey].split(\":\")[1].strip()\n",
    "    return data\n",
    "\n",
    "structured_data = clean_pay_stub_section(structured_data)\n",
    "\n",
    "\n",
    "def update_attribute_keys(data, section, key_mapping):\n",
    "    # Ensure the section exists in the data (either \"earnings\" or \"deductions\")\n",
    "    if section in data:\n",
    "        # Iterate over each type within the earnings or deductions section\n",
    "        for entry_type, attributes in data[section].items():\n",
    "            # Create a new dictionary to store the updated attributes\n",
    "            updated_attributes = {}\n",
    "            \n",
    "            # Loop through each attribute in that entry (e.g. 1, 2, 3)\n",
    "            for old_key, value in attributes.items():\n",
    "                # Map the old key (which is an integer) to the new descriptive key using key_mapping\n",
    "                if str(old_key) in key_mapping:  # Convert old_key to string to match the mapping\n",
    "                    new_key = key_mapping[str(old_key)]\n",
    "                else:\n",
    "                    new_key = old_key  # If no mapping is found, retain the old key\n",
    "                \n",
    "                # Update the dictionary with the new key\n",
    "                updated_attributes[new_key] = value\n",
    "\n",
    "            # Replace the old attributes with the updated attributes in the data\n",
    "            data[section][entry_type] = updated_attributes\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "paystub_final = update_attribute_keys(structured_data, \"earnings\", attribute_titles_earnings)\n",
    "paystub_final = structured_data = update_attribute_keys(structured_data, \"deductions\", attribute_titles_deductions)\n",
    "\n",
    "\n",
    "# Save the updated JSON structure back to the file\n",
    "with open('paystub_details.json', 'w') as json_file:\n",
    "    json.dump(paystub_final, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we get to the last part of our logical set of documents on a loan application process: the final loan agreement contract has been created and signed. A loan agreement contract is a legally binding document between a lender and a borrower that outlines the terms and conditions of a loan. This contract specifies the loan amount, interest rate, repayment schedule, and any other obligations or rights of both parties. It is crucial as it provides clarity and protection for both the lender and the borrower, ensuring that both parties understand their responsibilities and the consequences of default. Additionally, it serves as a legal record that can be referenced in case of disputes, helping to prevent misunderstandings and enforce the agreed-upon terms.\n",
    "\n",
    "The format of a loan agreement is, on its core, a text document that will not have a fixed structure. We should expect just as an input text document and therefore retrieve it as such. \n",
    "\n",
    "As we did in the previous steps, let's call the function that will retrieve the information inside the loanagreements folder, retrieving, once again, one single Loan Agreement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loan Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanagreement = read_json_files_from_blob(\"loanagreements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_data(json_data):\n",
    "    # Extract relevant text content from the JSON\n",
    "    content = []\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    paragraphs = json_data.get(\"paragraphs\", [])\n",
    "    for paragraph in paragraphs:\n",
    "        content.append(paragraph.get(\"text\", \"\").strip())\n",
    "\n",
    "    # Extract text from pages and lines\n",
    "    pages = json_data.get(\"pages\", [])\n",
    "    for page in pages:\n",
    "        for line in page.get(\"lines\", []):\n",
    "            content.append(line.get(\"text\", \"\").strip())\n",
    "\n",
    "    # Join all text content into a single string with spaces between components\n",
    "    plain_text_content = \" \".join(content)\n",
    "\n",
    "    # Extract Customer ID using regex\n",
    "    pattern = r\"Customer ID:\\s*(\\d+)\"\n",
    "    match = re.search(pattern, plain_text_content)\n",
    "    customer_id = match.group(1) if match else None\n",
    "    return plain_text_content, customer_id\n",
    "\n",
    "# Clean the JSON data and extract Customer ID\n",
    "loanagreement_structured, customer_id = clean_json_data(loanagreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Architecturing: From Retrieval to Upload (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Cosmos DB connection details from environment variables\n",
    "endpoint = os.getenv(\"COSMOS_DB_ENDPOINT\")\n",
    "key = os.getenv(\"COSMOS_DB_KEY\")\n",
    "\n",
    "def upload_text_to_cosmos_db(text_content, container_name):\n",
    "    # Check if the text is empty\n",
    "    if not text_content:\n",
    "        print(\"The text content is empty. No data to upload.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the Cosmos client\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    \n",
    "    try:\n",
    "        # Create or get the database\n",
    "        database = client.create_database_if_not_exists(id=\"ContosoDB\")\n",
    "        \n",
    "        # Create or get the container\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path=f\"/id\"),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while creating the database or container: {e.message}\")\n",
    "        return\n",
    "    \n",
    "    # Create a document with the text content and partition key\n",
    "    document = {\n",
    "        'id': str(customer_id),  # Generate a unique ID for the document\n",
    "        'content': text_content,  # Store the plain text as 'content'\n",
    "    }\n",
    "    \n",
    "    # Upload the document to the container\n",
    "    try:\n",
    "        container.create_item(body=document)\n",
    "        print(f\"Text content uploaded successfully with ID '{document['id']}' in Cosmos DB.\")\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while uploading the document: {e.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Pay Stubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while uploading the document: (Conflict) Entity with the specified id already exists in the system., {\"Summary\":{\"DirectCalls\":{\"(409, 0)\":1},\"RegionsContacted\":1,\"GatewayCalls\":{\"(200, 0)\":3,\"(304, 0)\":1}},\"name\":\"HandleDocumentRequest\",\"id\":\"893423d2-b6f0-497d-9323-c523cb902362\",\"start time\":\"09:35:27:661\",\"duration in milliseconds\":25.4598,\"data\":{\"Client Side Request Stats\":{\"Id\":\"AggregatedClientSideRequestStatistics\",\"ContactedReplicas\":[{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14311/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133668328813693539p\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14009/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133636440034569586s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14305/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133671076991305630s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14352/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133671076991305631s\"}],\"RegionsContacted\":[\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\"],\"FailedReplicas\":[],\"AddressResolutionStatistics\":[{\"StartTimeUTC\":\"2024-09-09T09:35:27.6736287Z\",\"EndTimeUTC\":\"2024-09-09T09:35:27.6753944Z\",\"TargetEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAOfKeS4%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\"}],\"StoreResponseStatistics\":[{\"ResponseTimeUTC\":\"2024-09-09T09:35:27.6866619Z\",\"ResourceType\":\"Document\",\"OperationType\":\"Create\",\"LocationEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\",\"StoreResult\":{\"ActivityId\":\"bcba1a11-48be-4848-9eb5-b08bc8bc1e12\",\"StatusCode\":\"Conflict\",\"SubStatusCode\":\"Unknown\",\"LSN\":2,\"PartitionKeyRangeId\":\"0\",\"GlobalCommittedLSN\":2,\"ItemLSN\":-1,\"UsingLocalLSN\":false,\"QuorumAckedLSN\":2,\"SessionToken\":\"-1#2\",\"CurrentWriteQuorum\":3,\"CurrentReplicaSetSize\":4,\"NumberOfReadRegions\":0,\"IsValid\":true,\"StorePhysicalAddress\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14311/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133668328813693539p\",\"RequestCharge\":5.1,\"RetryAfterInMs\":null,\"BELatencyInMs\":\"0.673\",\"transportRequestTimeline\":{\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-09T09:35:27.6755751Z\", \"durationInMs\": 0.0033},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-09T09:35:27.6755784Z\", \"durationInMs\": 9.5352},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-09T09:35:27.6851136Z\", \"durationInMs\": 0.0678},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-09T09:35:27.6851814Z\", \"durationInMs\": 1.4072},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-09T09:35:27.6865886Z\", \"durationInMs\": 0.0399},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-09T09:35:27.6866285Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"True\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-09T09:35:27.6841497Z\",\"lastSend\":\"2024-09-09T09:35:27.6841819Z\",\"lastReceive\":\"2024-09-09T09:35:27.6847912Z\"},\"requestSizeInBytes\":1734,\"requestBodySizeInBytes\":1297,\"responseMetadataSizeInBytes\":182,\"responseBodySizeInBytes\":65},\"TransportException\":null}}],\"HttpResponseStats\":[{\"StartTimeUTC\":\"2024-09-09T09:35:27.6642900Z\",\"DurationInMs\":3.0538,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/ContosoDB/colls/PayStubs\",\"ResourceType\":\"Collection\",\"HttpMethod\":\"GET\",\"ActivityId\":\"c0a2071f-5fc1-4212-976d-3307f649b975\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:27.6675604Z\",\"DurationInMs\":3.0661,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAOfKeS4=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"99b2fa92-53a8-4486-b564-169e52ae84d7\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:27.6707513Z\",\"DurationInMs\":2.7964,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAOfKeS4=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"3ccf7700-50bb-4e7d-897e-0115add15590\",\"StatusCode\":\"NotModified\",\"ReasonPhrase\":\"Not Modified\"},{\"StartTimeUTC\":\"2024-09-09T09:35:27.6736357Z\",\"DurationInMs\":1.6664,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAOfKeS4%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\",\"ResourceType\":\"Document\",\"HttpMethod\":\"GET\",\"ActivityId\":\"bcba1a11-48be-4848-9eb5-b08bc8bc1e12\",\"StatusCode\":\"OK\"}]}},\"children\":[{\"name\":\"Waiting for Initialization of client to complete\",\"id\":\"9e5cc21d-f3f9-4547-bcb7-e0bed72728a4\",\"start time\":\"09:35:27:661\",\"duration in milliseconds\":2.9568}]}, Windows/10.0.20348 cosmos-netstandard-sdk/3.18.0\n",
      "Code: Conflict\n",
      "Message: Entity with the specified id already exists in the system., {\"Summary\":{\"DirectCalls\":{\"(409, 0)\":1},\"RegionsContacted\":1,\"GatewayCalls\":{\"(200, 0)\":3,\"(304, 0)\":1}},\"name\":\"HandleDocumentRequest\",\"id\":\"893423d2-b6f0-497d-9323-c523cb902362\",\"start time\":\"09:35:27:661\",\"duration in milliseconds\":25.4598,\"data\":{\"Client Side Request Stats\":{\"Id\":\"AggregatedClientSideRequestStatistics\",\"ContactedReplicas\":[{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14311/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133668328813693539p\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14009/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133636440034569586s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14305/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133671076991305630s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14352/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133671076991305631s\"}],\"RegionsContacted\":[\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\"],\"FailedReplicas\":[],\"AddressResolutionStatistics\":[{\"StartTimeUTC\":\"2024-09-09T09:35:27.6736287Z\",\"EndTimeUTC\":\"2024-09-09T09:35:27.6753944Z\",\"TargetEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAOfKeS4%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\"}],\"StoreResponseStatistics\":[{\"ResponseTimeUTC\":\"2024-09-09T09:35:27.6866619Z\",\"ResourceType\":\"Document\",\"OperationType\":\"Create\",\"LocationEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\",\"StoreResult\":{\"ActivityId\":\"bcba1a11-48be-4848-9eb5-b08bc8bc1e12\",\"StatusCode\":\"Conflict\",\"SubStatusCode\":\"Unknown\",\"LSN\":2,\"PartitionKeyRangeId\":\"0\",\"GlobalCommittedLSN\":2,\"ItemLSN\":-1,\"UsingLocalLSN\":false,\"QuorumAckedLSN\":2,\"SessionToken\":\"-1#2\",\"CurrentWriteQuorum\":3,\"CurrentReplicaSetSize\":4,\"NumberOfReadRegions\":0,\"IsValid\":true,\"StorePhysicalAddress\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14311/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/a0b63464-4ac6-4154-b446-46dd29d7f17c/partitions/60028693-5916-41d6-9518-6bf11d7b20fb/replicas/133668328813693539p\",\"RequestCharge\":5.1,\"RetryAfterInMs\":null,\"BELatencyInMs\":\"0.673\",\"transportRequestTimeline\":{\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-09T09:35:27.6755751Z\", \"durationInMs\": 0.0033},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-09T09:35:27.6755784Z\", \"durationInMs\": 9.5352},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-09T09:35:27.6851136Z\", \"durationInMs\": 0.0678},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-09T09:35:27.6851814Z\", \"durationInMs\": 1.4072},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-09T09:35:27.6865886Z\", \"durationInMs\": 0.0399},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-09T09:35:27.6866285Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"True\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-09T09:35:27.6841497Z\",\"lastSend\":\"2024-09-09T09:35:27.6841819Z\",\"lastReceive\":\"2024-09-09T09:35:27.6847912Z\"},\"requestSizeInBytes\":1734,\"requestBodySizeInBytes\":1297,\"responseMetadataSizeInBytes\":182,\"responseBodySizeInBytes\":65},\"TransportException\":null}}],\"HttpResponseStats\":[{\"StartTimeUTC\":\"2024-09-09T09:35:27.6642900Z\",\"DurationInMs\":3.0538,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/ContosoDB/colls/PayStubs\",\"ResourceType\":\"Collection\",\"HttpMethod\":\"GET\",\"ActivityId\":\"c0a2071f-5fc1-4212-976d-3307f649b975\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:27.6675604Z\",\"DurationInMs\":3.0661,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAOfKeS4=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"99b2fa92-53a8-4486-b564-169e52ae84d7\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:27.6707513Z\",\"DurationInMs\":2.7964,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAOfKeS4=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"3ccf7700-50bb-4e7d-897e-0115add15590\",\"StatusCode\":\"NotModified\",\"ReasonPhrase\":\"Not Modified\"},{\"StartTimeUTC\":\"2024-09-09T09:35:27.6736357Z\",\"DurationInMs\":1.6664,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAOfKeS4%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\",\"ResourceType\":\"Document\",\"HttpMethod\":\"GET\",\"ActivityId\":\"bcba1a11-48be-4848-9eb5-b08bc8bc1e12\",\"StatusCode\":\"OK\"}]}},\"children\":[{\"name\":\"Waiting for Initialization of client to complete\",\"id\":\"9e5cc21d-f3f9-4547-bcb7-e0bed72728a4\",\"start time\":\"09:35:27:661\",\"duration in milliseconds\":2.9568}]}, Windows/10.0.20348 cosmos-netstandard-sdk/3.18.0\n"
     ]
    }
   ],
   "source": [
    "upload_text_to_cosmos_db(paystub_final, \"PayStubs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Loan Forms - to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content uploaded successfully with ID '100002' in Cosmos DB.\n"
     ]
    }
   ],
   "source": [
    "upload_text_to_cosmos_db(json_data, \"LoanForms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Loan Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while uploading the document: (Conflict) Entity with the specified id already exists in the system., {\"Summary\":{\"DirectCalls\":{\"(409, 0)\":1},\"RegionsContacted\":1,\"GatewayCalls\":{\"(200, 0)\":3,\"(304, 0)\":1}},\"name\":\"HandleDocumentRequest\",\"id\":\"13fea76d-9bbb-42c2-a0aa-20dae1e3ca08\",\"start time\":\"09:35:31:689\",\"duration in milliseconds\":24.5944,\"data\":{\"Client Side Request Stats\":{\"Id\":\"AggregatedClientSideRequestStatistics\",\"ContactedReplicas\":[{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14073/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945831p\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14311/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945832s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14318/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723023039475s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14335/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945830s\"}],\"RegionsContacted\":[\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\"],\"FailedReplicas\":[],\"AddressResolutionStatistics\":[{\"StartTimeUTC\":\"2024-09-09T09:35:31.7011778Z\",\"EndTimeUTC\":\"2024-09-09T09:35:31.7030060Z\",\"TargetEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAKWHPTo%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\"}],\"StoreResponseStatistics\":[{\"ResponseTimeUTC\":\"2024-09-09T09:35:31.7142600Z\",\"ResourceType\":\"Document\",\"OperationType\":\"Create\",\"LocationEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\",\"StoreResult\":{\"ActivityId\":\"bd3a9fe9-16e7-49bf-b98c-3a611ed95912\",\"StatusCode\":\"Conflict\",\"SubStatusCode\":\"Unknown\",\"LSN\":2,\"PartitionKeyRangeId\":\"0\",\"GlobalCommittedLSN\":2,\"ItemLSN\":-1,\"UsingLocalLSN\":false,\"QuorumAckedLSN\":2,\"SessionToken\":\"-1#2\",\"CurrentWriteQuorum\":3,\"CurrentReplicaSetSize\":4,\"NumberOfReadRegions\":0,\"IsValid\":true,\"StorePhysicalAddress\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14073/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945831p\",\"RequestCharge\":1.67,\"RetryAfterInMs\":null,\"BELatencyInMs\":\"0.773\",\"transportRequestTimeline\":{\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-09T09:35:31.7031865Z\", \"durationInMs\": 0.0023},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-09T09:35:31.7031888Z\", \"durationInMs\": 9.4399},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-09T09:35:31.7126287Z\", \"durationInMs\": 0.0663},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-09T09:35:31.7126950Z\", \"durationInMs\": 1.4937},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-09T09:35:31.7141887Z\", \"durationInMs\": 0.031},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-09T09:35:31.7142197Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"True\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-09T09:35:31.7117353Z\",\"lastSend\":\"2024-09-09T09:35:31.7117688Z\",\"lastReceive\":\"2024-09-09T09:35:31.7122895Z\"},\"requestSizeInBytes\":4048,\"requestBodySizeInBytes\":3607,\"responseMetadataSizeInBytes\":182,\"responseBodySizeInBytes\":65},\"TransportException\":null}}],\"HttpResponseStats\":[{\"StartTimeUTC\":\"2024-09-09T09:35:31.6912926Z\",\"DurationInMs\":3.4297,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/ContosoDB/colls/LoanAgreements\",\"ResourceType\":\"Collection\",\"HttpMethod\":\"GET\",\"ActivityId\":\"b8164a62-3776-4784-ad31-7e5141faed74\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:31.6949654Z\",\"DurationInMs\":3.1377,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAKWHPTo=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"f9cd8046-27a8-4c1c-aff0-20780626261c\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:31.6982048Z\",\"DurationInMs\":2.8878,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAKWHPTo=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"c91e7693-6c54-42fe-b5c6-f4393946ec37\",\"StatusCode\":\"NotModified\",\"ReasonPhrase\":\"Not Modified\"},{\"StartTimeUTC\":\"2024-09-09T09:35:31.7011947Z\",\"DurationInMs\":1.7095,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAKWHPTo%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\",\"ResourceType\":\"Document\",\"HttpMethod\":\"GET\",\"ActivityId\":\"bd3a9fe9-16e7-49bf-b98c-3a611ed95912\",\"StatusCode\":\"OK\"}]}},\"children\":[{\"name\":\"Waiting for Initialization of client to complete\",\"id\":\"09865d84-de3a-4b82-84db-d12088427f27\",\"start time\":\"09:35:31:689\",\"duration in milliseconds\":1.5025}]}, Windows/10.0.20348 cosmos-netstandard-sdk/3.18.0\n",
      "Code: Conflict\n",
      "Message: Entity with the specified id already exists in the system., {\"Summary\":{\"DirectCalls\":{\"(409, 0)\":1},\"RegionsContacted\":1,\"GatewayCalls\":{\"(200, 0)\":3,\"(304, 0)\":1}},\"name\":\"HandleDocumentRequest\",\"id\":\"13fea76d-9bbb-42c2-a0aa-20dae1e3ca08\",\"start time\":\"09:35:31:689\",\"duration in milliseconds\":24.5944,\"data\":{\"Client Side Request Stats\":{\"Id\":\"AggregatedClientSideRequestStatistics\",\"ContactedReplicas\":[{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14073/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945831p\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14311/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945832s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14318/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723023039475s\"},{\"Count\":1,\"Uri\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14335/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945830s\"}],\"RegionsContacted\":[\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\"],\"FailedReplicas\":[],\"AddressResolutionStatistics\":[{\"StartTimeUTC\":\"2024-09-09T09:35:31.7011778Z\",\"EndTimeUTC\":\"2024-09-09T09:35:31.7030060Z\",\"TargetEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAKWHPTo%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\"}],\"StoreResponseStatistics\":[{\"ResponseTimeUTC\":\"2024-09-09T09:35:31.7142600Z\",\"ResourceType\":\"Document\",\"OperationType\":\"Create\",\"LocationEndpoint\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/\",\"StoreResult\":{\"ActivityId\":\"bd3a9fe9-16e7-49bf-b98c-3a611ed95912\",\"StatusCode\":\"Conflict\",\"SubStatusCode\":\"Unknown\",\"LSN\":2,\"PartitionKeyRangeId\":\"0\",\"GlobalCommittedLSN\":2,\"ItemLSN\":-1,\"UsingLocalLSN\":false,\"QuorumAckedLSN\":2,\"SessionToken\":\"-1#2\",\"CurrentWriteQuorum\":3,\"CurrentReplicaSetSize\":4,\"NumberOfReadRegions\":0,\"IsValid\":true,\"StorePhysicalAddress\":\"rntbd://cdb-ms-prod-swedencentral1-be62.documents.azure.com:14073/apps/928c0c85-23d6-43f2-8764-1493aa2e804f/services/f6e1c01a-d183-4817-8dfe-d4f0b9ea4283/partitions/d69f5c80-2269-4525-aed5-d7162725abda/replicas/133698723051945831p\",\"RequestCharge\":1.67,\"RetryAfterInMs\":null,\"BELatencyInMs\":\"0.773\",\"transportRequestTimeline\":{\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-09T09:35:31.7031865Z\", \"durationInMs\": 0.0023},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-09T09:35:31.7031888Z\", \"durationInMs\": 9.4399},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-09T09:35:31.7126287Z\", \"durationInMs\": 0.0663},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-09T09:35:31.7126950Z\", \"durationInMs\": 1.4937},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-09T09:35:31.7141887Z\", \"durationInMs\": 0.031},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-09T09:35:31.7142197Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"True\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-09T09:35:31.7117353Z\",\"lastSend\":\"2024-09-09T09:35:31.7117688Z\",\"lastReceive\":\"2024-09-09T09:35:31.7122895Z\"},\"requestSizeInBytes\":4048,\"requestBodySizeInBytes\":3607,\"responseMetadataSizeInBytes\":182,\"responseBodySizeInBytes\":65},\"TransportException\":null}}],\"HttpResponseStats\":[{\"StartTimeUTC\":\"2024-09-09T09:35:31.6912926Z\",\"DurationInMs\":3.4297,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/ContosoDB/colls/LoanAgreements\",\"ResourceType\":\"Collection\",\"HttpMethod\":\"GET\",\"ActivityId\":\"b8164a62-3776-4784-ad31-7e5141faed74\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:31.6949654Z\",\"DurationInMs\":3.1377,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAKWHPTo=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"f9cd8046-27a8-4c1c-aff0-20780626261c\",\"StatusCode\":\"OK\"},{\"StartTimeUTC\":\"2024-09-09T09:35:31.6982048Z\",\"DurationInMs\":2.8878,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com/dbs/otInAA==/colls/otInAKWHPTo=/pkranges\",\"ResourceType\":\"PartitionKeyRange\",\"HttpMethod\":\"GET\",\"ActivityId\":\"c91e7693-6c54-42fe-b5c6-f4393946ec37\",\"StatusCode\":\"NotModified\",\"ReasonPhrase\":\"Not Modified\"},{\"StartTimeUTC\":\"2024-09-09T09:35:31.7011947Z\",\"DurationInMs\":1.7095,\"RequestUri\":\"https://cosmosweaihack-swedencentral.sql.cosmos.azure.com//addresses/?$resolveFor=dbs%2fotInAA%3d%3d%2fcolls%2fotInAKWHPTo%3d%2fdocs&$filter=protocol eq rntbd&$partitionKeyRangeIds=0\",\"ResourceType\":\"Document\",\"HttpMethod\":\"GET\",\"ActivityId\":\"bd3a9fe9-16e7-49bf-b98c-3a611ed95912\",\"StatusCode\":\"OK\"}]}},\"children\":[{\"name\":\"Waiting for Initialization of client to complete\",\"id\":\"09865d84-de3a-4b82-84db-d12088427f27\",\"start time\":\"09:35:31:689\",\"duration in milliseconds\":1.5025}]}, Windows/10.0.20348 cosmos-netstandard-sdk/3.18.0\n"
     ]
    }
   ],
   "source": [
    "upload_text_to_cosmos_db(loanagreement_structured, \"LoanAgreements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
