{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Modelling: From Retrieval to Upload (1/2)\n",
    "\n",
    "\n",
    "In this step, we will structure the data retrieved from Azure Document Intelligence (ADI) into the right format to be read by our systems in subsequent steps. \n",
    "\n",
    "The data will be outputted from the ADI as a JSON file, and it is our role to process and organize it. Some of the data will be structured into tables, while other data will be formatted as text. This step ensures that the extracted information is organized in a meaningful way for further analysis and usage.\n",
    "\n",
    "As stated before, we need to make sure that our Function will know how to process:\n",
    "- **Loan Forms:** Extract relevant details such as borrower information, loan amounts, and terms.\n",
    "- **Loan Contract:** Identify and parse key contract elements like clauses, signatures, and dates.\n",
    "- **Pay Stubs:** Retrieve data such as employee details, earnings, deductions, and net pay.\n",
    "\n",
    "Not all customers will have provided all types of content, and during this Challenge we will be only be processing one file. We will combine in the next challenge the capabilities of a trigger, which will, at a time, also process one single document.\n",
    "\n",
    "Due to the nature of this challenge, we will separate this challenge in the 3 different types of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Forms \n",
    "\n",
    "The first step to get a Loan, is to fill out a form with some basic details, such as customer ID, Full Name, Date Of Birth, etc, therefore, that's where we will start. \n",
    "\n",
    "This particular document combines text and tables, that the ADI capabilities allow you to extract as also separate capabilities.\n",
    "\n",
    "To first start our analysis, let's create a function that will load the documents inside a folder inside a container that is, on its turn, inside our designated Storage Account. In our particular step, inside the folder of the Loan Forms, we will retrieve one Loan Form for us to analyse. \n",
    "\n",
    "We will consequently use this same function to access other folders that will contain other type of documents.\n",
    "\n",
    "**Question: why are we not batch-analysing documents?**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def read_json_files_from_blob(folder_path):\n",
    "    # Retrieve the connection string from the environment variables\n",
    "    connection_string = os.getenv('connection_string')\n",
    "\n",
    "    # Ensure the connection string is not None\n",
    "    if connection_string is None:\n",
    "        raise ValueError(\"The connection string environment variable is not set.\")\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(\"bankdetail\")\n",
    "\n",
    "    # List all blobs in the specified folder\n",
    "    blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "\n",
    "    # Filter out JSON files and read their contents\n",
    "    for blob in blob_list:\n",
    "        if blob.name.endswith('.json'):\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            data = json.loads(blob_data)\n",
    "            # print(f\"Contents of {blob.name}:\")\n",
    "            # print(json.dumps(data, indent=2))\n",
    "            # print(\"\\n\")\n",
    "            return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is to call our function and pass the name of our folder as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanform = read_json_files_from_blob(\"loanform\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a function that will process the loan application form data. This function will take the loan application form data as input and return the result of the loan application processing. Our input data is a JSON file that is composed of both text and tables, and we will need to treat both of them seperatly.  \n",
    "\n",
    "The function will perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_structured_tables function processes a list of tables by initializing and populating them with cell content, combining specific rows for tables with 3 rows and 5 columns, and returning the structured tables along with any combined rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    combined_rows = []\n",
    "    \n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        # Combine the last row with the previous one if the table has 5 columns and 3 rows\n",
    "        if row_count == 3 and column_count == 5:\n",
    "            combined_row = [structured_table[1][i] + \" \" + structured_table[2][i] for i in range(column_count)]\n",
    "            structured_table[1] = combined_row\n",
    "            structured_table = structured_table[:2]\n",
    "            combined_rows.append(combined_row)\n",
    "        \n",
    "        # Append the structured table to the list\n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables, combined_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean_form_recognizer_result function processes form recognizer output by extracting text data while ignoring lines containing the word \"table\", retaining only the \"text\" key in each line, and creating structured tables from the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_data = []\n",
    "    table_encountered = False\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                table_encountered = True\n",
    "                continue  # Skip the line if \"table\" is in the text\n",
    "            \n",
    "            if not table_encountered:\n",
    "                # Collect the \"text\" information\n",
    "                text_data.append(line.get(\"text\", \"\"))\n",
    "            \n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables, combined_rows = create_structured_tables(data.get(\"tables\", []))\n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"combined_rows\"] = combined_rows\n",
    "    data[\"text_data\"] = text_data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables_to_dataframes function converts a list of structured tables into a list of pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now retrieved both our table with the structured desired and the text that comes out of our files. However, this function doesn't have the data as structured as we need it to be. \n",
    "\n",
    "As an example, we have by now extracted a key-value pair which keys is \"text\" with the value \"Contact Number: (555) 234-5678\". What we will need to define now is to remove the name of the field, and start composing the key-value pair that would be key \"Contact Number:\" and value \"(555) 234-5678\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "def clean_loan_application_file(text):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    # Extract the category from the first three words\n",
    "    category_match = re.search(r'(\\w+\\s+\\w+\\s+\\w+)', text)\n",
    "    if category_match:\n",
    "        cleaned_data['Category'] = category_match.group(1)\n",
    "    \n",
    "    # Extract Applicant Information\n",
    "    applicant_info = re.search(r'Applicant Information(.*?)Employment and Income Details', text, re.DOTALL)\n",
    "    if applicant_info:\n",
    "        applicant_info_text = applicant_info.group(1)\n",
    "        cleaned_data['Applicant Information'] = {\n",
    "            'Full Name': re.search(r'Full Name:\\s*(.*?)Date of Birth:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Date of Birth': re.search(r'Date of Birth:\\s*(.*?)Social Security Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Social Security Number': re.search(r'Social Security Number:\\s*(.*?)Contact Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Contact Number': re.search(r'Contact Number:\\s*(.*?)Email Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Email Address': re.search(r'Email Address:\\s*(.*?)Physical Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Physical Address': re.search(r'Physical Address:\\s*(.*)', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    # Extract Loan Information\n",
    "    loan_info = re.search(r'Loan Information(.*)', text, re.DOTALL)\n",
    "    if loan_info:\n",
    "        loan_info_text = loan_info.group(1)\n",
    "        cleaned_data['Loan Information'] = {\n",
    "            'Loan Amount Requested': re.search(r'Loan Amount Requested:\\s*\\$?(.*?)Purpose of Loan:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Purpose of Loan': re.search(r'Purpose of Loan:\\s*(.*?)Loan Term Desired:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Loan Term Desired': re.search(r'Loan Term Desired:\\s*(.*)', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to combine extracted tables and text\n",
    "def process_loan_application(data):\n",
    "    # Clean form recognizer result to extract structured tables and text\n",
    "    cleaned_data = clean_form_recognizer_result(data)\n",
    "    \n",
    "    # Convert extracted tables to dataframes\n",
    "    dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"]) #tabela bem retirada\n",
    "    # print(dataframes)\n",
    "    # Combine all table dataframes into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True) #tabela bem retirada\n",
    "    combined_df.columns = combined_df.iloc[0]\n",
    "    combined_df = combined_df[1:]\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    combined_df.rename(columns={\"Contact Number\": \"Employer Contact Number\"}, inplace=True)\n",
    "    combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "    # Clean the extracted text using regex\n",
    "    combined_text = ' '.join(cleaned_data['text_data'])\n",
    "    text_data = clean_loan_application_file(combined_text)\n",
    "    #print(text_data) #tudo menos a tabela perfeitamente mas em dict\n",
    "\n",
    "    def clean_loan_application(data):\n",
    "    # Extract applicant and loan info\n",
    "        applicant_info = data['Applicant Information']\n",
    "        loan_info = data['Loan Information']\n",
    "        \n",
    "        # Combine keys and values for the two categories\n",
    "        fields = list(applicant_info.keys()) + list(loan_info.keys())\n",
    "        values = list(applicant_info.values()) + list(loan_info.values())\n",
    "        \n",
    "        # Create the 2x10 DataFrame without 'Category'\n",
    "        df = pd.DataFrame({\n",
    "            'Field': fields,\n",
    "            'Value': values\n",
    "        })\n",
    "        \n",
    "        return df.set_index('Field').T\n",
    "\n",
    "    df_cleaned = clean_loan_application(text_data)\n",
    "\n",
    "    # Convert the text data to a DataFrame\n",
    "    text_df = pd.DataFrame(df_cleaned)\n",
    "\n",
    "    # Concatenate the text dataframe with the tables dataframe\n",
    "    final_df = pd.concat([text_df, combined_df], axis=1)\n",
    "\n",
    "    def remove_empty_cells_and_push_up(df):\n",
    "        for column in df.columns:\n",
    "            non_empty_values = df[column].replace('', pd.NA).dropna().values\n",
    "            df[column] = pd.Series(non_empty_values).reindex(df.index, fill_value='')\n",
    "        return df\n",
    "\n",
    "    return remove_empty_cells_and_push_up(final_df)\n",
    "\n",
    "# Process the loan application\n",
    "loanform = process_loan_application(loanform).iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Save the final dataframe to an Excel file\n",
    "# loanform.to_excel('loanform.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay Stub \n",
    "\n",
    "As part of some loan applications, the pay stub is a required document. The pay stub is a document that outlines the details of an employee’s income. It contains the employee’s wages earned, applicable deductions and total gross pay, and net pay for the pay period. A pay stub will provide Contoso bank with crucial information about not only a person’s income and employment stability, which helps assess their ability to repay the loan. It also verifies the applicant’s financial credibility and ensures that their reported income matches their actual earnings.\n",
    "\n",
    "When processing a Pay Stub, we will have similar challenges as we previously did on the Loan Forms. These particular documents combine text and contrary to the previous use case, more than 1 table, Once again, the ADI capabilities allows you to extract these 2 types of entities as also separate capabilities.\n",
    "\n",
    "As we've previously create a the function that will load the documents inside a designated folder, all we have to do now is to retrieve all the information inside the paystub folder, we will retrieve one single Loan Form for us to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paystub = read_json_files_from_blob(\"paystubs\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0             1       2                 3  \\\n",
      "0     Description  Hours Worked    Rate  Current Earnings   \n",
      "1     Regular Pay           160  $28.65         $4,583.33   \n",
      "2    Overtime Pay             5  $42.98           $214.90   \n",
      "3           Bonus           N/A     N/A           $250.00   \n",
      "4  Total Earnings                               $5,048.23   \n",
      "\n",
      "                       4  \n",
      "0  Year-to-Date Earnings  \n",
      "1             $32,083.31  \n",
      "2              $1,289.40  \n",
      "3              $1,750.00  \n",
      "4             $35,122.71  \n",
      "                     0               1                    2\n",
      "0          Description  Current Amount  Year-to-Date Amount\n",
      "1          Federal Tax         $800.00            $5,600.00\n",
      "2            State Tax         $200.00            $1,400.00\n",
      "3      Social Security         $314.99            $2,204.93\n",
      "4             Medicare          $73.66              $515.49\n",
      "5  401(k) Contribution         $250.00            $1,750.00\n",
      "6     Health Insurance         $150.00            $1,050.00\n",
      "7    Total Deductions:       $1,788.65           $12,520.42\n"
     ]
    }
   ],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                continue  # Keep everything if \"table\" is in the text\n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables = create_structured_tables(data.get(\"tables\", []))\n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables\n",
    "\n",
    "import pandas as pd\n",
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "cleaned_data = clean_form_recognizer_result(paystub)\n",
    "dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"])\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for df in dataframes:\n",
    "    print(df)\n",
    "    df_list.append(df)\n",
    "\n",
    "paystub = pd.concat(df_list, ignore_index=True)\n",
    "# combined_df.to_excel('dataframes_combined.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Agreement\n",
    "\n",
    "Now we get to the last part of our logical set of documents on a loan application process: the final loan agreement contract has been created and signed. A loan agreement contract is a legally binding document between a lender and a borrower that outlines the terms and conditions of a loan. This contract specifies the loan amount, interest rate, repayment schedule, and any other obligations or rights of both parties. It is crucial as it provides clarity and protection for both the lender and the borrower, ensuring that both parties understand their responsibilities and the consequences of default. Additionally, it serves as a legal record that can be referenced in case of disputes, helping to prevent misunderstandings and enforce the agreed-upon terms.\n",
    "\n",
    "The format of a loan agreement is, on its core, a text document that will not have a fixed structure. We should expect just as an input text document and therefore retrieve it as such. \n",
    "\n",
    "As we did in the previous steps, let's call the function that will retrieve the information inside the loanagreements folder, retrieving, once again, one single Loan Agreement.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loan Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanagreement = read_json_files_from_blob(\"loanagreements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_data(json_data):\n",
    "    # Extract relevant text content from the JSON\n",
    "    content = []\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    paragraphs = json_data.get(\"paragraphs\", [])\n",
    "    for paragraph in paragraphs:\n",
    "        content.append(paragraph.get(\"text\", \"\").strip())\n",
    "\n",
    "    # Extract text from pages and lines\n",
    "    pages = json_data.get(\"pages\", [])\n",
    "    for page in pages:\n",
    "        for line in page.get(\"lines\", []):\n",
    "            content.append(line.get(\"text\", \"\").strip())\n",
    "\n",
    "    # Join all text content into a single string with spaces between components\n",
    "    plain_text_content = \" \".join(content)\n",
    "\n",
    "    return plain_text_content\n",
    "\n",
    "# Clean the JSON data\n",
    "loanagreement = clean_json_data(loanagreement)\n",
    "\n",
    "# Print the cleaned data\n",
    "# print(json.dumps(cleaned_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Architecturing: From Retrieval to Upload (2/2)\n",
    "\n",
    "Adicionar o Customer\n",
    "Upload to Cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON content uploaded successfully as 'loanform1' in Cosmos DB.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"COSMOS_DB_ENDPOINT\")\n",
    "key = os.getenv(\"COSMOS_DB_KEY\")\n",
    "database_name = os.getenv(\"COSMOS_DB_DATABASE_NAME\")\n",
    "container_name = os.getenv(\"COSMOS_DB_CONTAINER_NAME\")\n",
    "file_name = os.getenv(\"FILE_NAME\")\n",
    "\n",
    "def upload_dataframe_to_cosmos_db(df, file_name, endpoint, key, database_name, container_name):\n",
    "    # Initialize the Cosmos client\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    \n",
    "    # Create or get the database\n",
    "    database = client.create_database_if_not_exists(id=database_name)\n",
    "    \n",
    "    # Create or get the container\n",
    "    container = database.create_container_if_not_exists(\n",
    "        id=container_name,\n",
    "        partition_key=PartitionKey(path=\"/id\"),\n",
    "        offer_throughput=400\n",
    "    )\n",
    "    \n",
    "    # Convert DataFrame to JSON string\n",
    "    json_content = df.to_json(orient='records')\n",
    "    \n",
    "    # Parse the JSON string\n",
    "    json_data = json.loads(json_content)\n",
    "    \n",
    "    # Create a document with the JSON content\n",
    "    document = {\n",
    "        'id': file_name,\n",
    "        'content': json_data\n",
    "    }\n",
    "    \n",
    "    # Upload the document to the container\n",
    "    try:\n",
    "        container.create_item(body=document)\n",
    "        print(f\"JSON content uploaded successfully as '{file_name}' in Cosmos DB.\")\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred: {e.message}\")\n",
    "\n",
    "# Example usage\n",
    "# df = pd.read_csv('path_to_your_csv_file.csv')\n",
    "upload_dataframe_to_cosmos_db(loanform, \"loanform1\", endpoint, key, database_name, container_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
